{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srinjoy-das/.local/lib/python3.11/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "class TradingStrategies:\n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Initialize with OHLC data\n",
    "        data: DataFrame with columns ['Date', 'Symbol', 'Open', 'High', 'Low', 'Close', 'Volume', etc.]\n",
    "        \"\"\"\n",
    "        self.data = data.copy()\n",
    "        self.symbols = data['Symbol'].unique()\n",
    "        self.dates = sorted(data['Date'].unique())\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        \"\"\"Prepare data in pivot format for easier calculations\"\"\"\n",
    "        self.close_prices = self.data.pivot(index='Date', columns='Symbol', values='Close')\n",
    "        self.high_prices = self.data.pivot(index='Date', columns='Symbol', values='High')\n",
    "        self.low_prices = self.data.pivot(index='Date', columns='Symbol', values='Low')\n",
    "        self.volume_data = self.data.pivot(index='Date', columns='Symbol', values='Volume')\n",
    "        \n",
    "    def task1_Strategy1(self, current_date_idx):\n",
    "        \"\"\"\n",
    "        Strategy 1: Average Weekly Returns\n",
    "        - Calculate weekly returns over past 50 weeks\n",
    "        - Top 6 stocks get negative weights (-1 total), bottom 6 get positive weights (+1 total)\n",
    "        \"\"\"\n",
    "        if current_date_idx < 250:  # Need at least 50 weeks of data\n",
    "            return pd.Series(0, index=self.symbols)\n",
    "        \n",
    "        # Get data up to current date (no lookahead bias)\n",
    "        end_idx = current_date_idx\n",
    "        start_idx = max(0, end_idx - 250)  # 50 weeks * 5 days\n",
    "        \n",
    "        weekly_returns = {}\n",
    "        \n",
    "        for symbol in self.symbols:\n",
    "            prices = self.close_prices.iloc[start_idx:end_idx][symbol].dropna()\n",
    "            if len(prices) < 250:\n",
    "                weekly_returns[symbol] = 0\n",
    "                continue\n",
    "                \n",
    "            # Calculate weekly returns for complete weeks only\n",
    "            weekly_rets = []\n",
    "            for week_start in range(0, len(prices) - 5, 5):\n",
    "                if week_start + 5 <= len(prices):\n",
    "                    week_end_price = prices.iloc[week_start + 4]\n",
    "                    prev_week_end_price = prices.iloc[week_start - 1] if week_start > 0 else 1.0\n",
    "                    weekly_ret = (week_end_price - prev_week_end_price) / prev_week_end_price\n",
    "                    weekly_rets.append(weekly_ret)\n",
    "            \n",
    "            weekly_returns[symbol] = np.mean(weekly_rets) if weekly_rets else 0\n",
    "        \n",
    "        # Rank and assign weights\n",
    "        returns_series = pd.Series(weekly_returns)\n",
    "        ranked_stocks = returns_series.sort_values(ascending=False)\n",
    "        \n",
    "        weights = pd.Series(0.0, index=self.symbols)\n",
    "        \n",
    "        # Top 6 get negative weights\n",
    "        top_6 = ranked_stocks.head(6).index\n",
    "        weights[top_6] = -1/6\n",
    "        \n",
    "        # Bottom 6 get positive weights\n",
    "        bottom_6 = ranked_stocks.tail(6).index\n",
    "        weights[bottom_6] = 1/6\n",
    "        \n",
    "        return weights\n",
    "    \n",
    "    def task1_Strategy2(self, current_date_idx):\n",
    "        \"\"\"\n",
    "        Strategy 2: Mean Reversion (SMA vs LMA)\n",
    "        - Compare 5-day SMA vs 30-day LMA\n",
    "        - Top 5 get negative weights (-1 total), bottom 5 get positive weights (+1 total)\n",
    "        \"\"\"\n",
    "        if current_date_idx < 30:\n",
    "            return pd.Series(0, index=self.symbols)\n",
    "        \n",
    "        relative_positions = {}\n",
    "        \n",
    "        for symbol in self.symbols:\n",
    "            prices = self.close_prices.iloc[max(0, current_date_idx-30):current_date_idx][symbol].dropna()\n",
    "            \n",
    "            if len(prices) < 30:\n",
    "                relative_positions[symbol] = 0\n",
    "                continue\n",
    "                \n",
    "            lma = prices.tail(30).mean()  # 30-day LMA\n",
    "            sma = prices.tail(5).mean()   # 5-day SMA\n",
    "            \n",
    "            relative_pos = (sma - lma) / lma if lma != 0 else 0\n",
    "            relative_positions[symbol] = relative_pos\n",
    "        \n",
    "        # Rank and assign weights\n",
    "        rel_pos_series = pd.Series(relative_positions)\n",
    "        ranked_stocks = rel_pos_series.sort_values(ascending=False)\n",
    "        \n",
    "        weights = pd.Series(0.0, index=self.symbols)\n",
    "        \n",
    "        # Top 5 get negative weights\n",
    "        top_5 = ranked_stocks.head(5).index\n",
    "        weights[top_5] = -1/5\n",
    "        \n",
    "        # Bottom 5 get positive weights\n",
    "        bottom_5 = ranked_stocks.tail(5).index\n",
    "        weights[bottom_5] = 1/5\n",
    "        \n",
    "        return weights\n",
    "    \n",
    "    def task1_Strategy3(self, current_date_idx):\n",
    "        \"\"\"\n",
    "        Strategy 3: Rate of Change (ROC)\n",
    "        - Calculate 7-day ROC\n",
    "        - Rank based on ROC (implementation needs completion based on weight assignment)\n",
    "        \"\"\"\n",
    "        if current_date_idx < 7:\n",
    "            return pd.Series(0, index=self.symbols)\n",
    "        \n",
    "        roc_values = {}\n",
    "        \n",
    "        for symbol in self.symbols:\n",
    "            current_price = self.close_prices.iloc[current_date_idx-1][symbol]\n",
    "            price_7_days_ago = self.close_prices.iloc[current_date_idx-8][symbol]\n",
    "            \n",
    "            if pd.notna(current_price) and pd.notna(price_7_days_ago) and price_7_days_ago != 0:\n",
    "                roc = 100 * (current_price - price_7_days_ago) / price_7_days_ago\n",
    "                roc_values[symbol] = roc\n",
    "            else:\n",
    "                roc_values[symbol] = 0\n",
    "        \n",
    "        # Rank and assign weights (assuming similar pattern to other strategies)\n",
    "        roc_series = pd.Series(roc_values)\n",
    "        ranked_stocks = roc_series.sort_values(ascending=False)\n",
    "        \n",
    "        weights = pd.Series(0.0, index=self.symbols)\n",
    "        \n",
    "        # Top 5 get negative weights, bottom 5 get positive weights\n",
    "        top_5 = ranked_stocks.head(5).index\n",
    "        weights[top_5] = -1/5\n",
    "        \n",
    "        bottom_5 = ranked_stocks.tail(5).index\n",
    "        weights[bottom_5] = 1/5\n",
    "        \n",
    "        return weights\n",
    "    \n",
    "    def task1_Strategy4(self, current_date_idx):\n",
    "        \"\"\"\n",
    "        Strategy 4: Support and Resistance\n",
    "        - Calculate support/resistance using 21-day SMA Â± 3*std\n",
    "        - Complex weight assignment based on proximity\n",
    "        \"\"\"\n",
    "        if current_date_idx < 21:\n",
    "            return pd.Series(0, index=self.symbols)\n",
    "        \n",
    "        proximities = {}\n",
    "        \n",
    "        for symbol in self.symbols:\n",
    "            prices = self.close_prices.iloc[max(0, current_date_idx-21):current_date_idx][symbol].dropna()\n",
    "            \n",
    "            if len(prices) < 21:\n",
    "                proximities[symbol] = {'support': 0, 'resistance': 0}\n",
    "                continue\n",
    "            \n",
    "            sma_21 = prices.mean()\n",
    "            std_21 = prices.std()\n",
    "            \n",
    "            resistance = sma_21 + 3 * std_21\n",
    "            support = sma_21 - 3 * std_21\n",
    "            current_price = prices.iloc[-1]\n",
    "            \n",
    "            prox_resistance = (current_price - resistance) / resistance if resistance != 0 else 0\n",
    "            prox_support = (current_price - support) / support if support != 0 else 0\n",
    "            \n",
    "            proximities[symbol] = {'support': prox_support, 'resistance': prox_resistance}\n",
    "        \n",
    "        # Complex weight assignment\n",
    "        support_prox = {symbol: proximities[symbol]['support'] for symbol in self.symbols}\n",
    "        resistance_prox = {symbol: proximities[symbol]['resistance'] for symbol in self.symbols}\n",
    "        \n",
    "        # Rank by proximity to support (increasing order)\n",
    "        support_ranked = sorted(support_prox.items(), key=lambda x: x[1])\n",
    "        top_4_support = [x[0] for x in support_ranked[:4]]\n",
    "        \n",
    "        # Remaining stocks ranked by proximity to resistance (decreasing order)\n",
    "        remaining_stocks = [s for s in self.symbols if s not in top_4_support]\n",
    "        resistance_remaining = {s: resistance_prox[s] for s in remaining_stocks}\n",
    "        resistance_ranked = sorted(resistance_remaining.items(), key=lambda x: x[1], reverse=True)\n",
    "        top_4_resistance = [x[0] for x in resistance_ranked[:4]]\n",
    "        \n",
    "        weights = pd.Series(0.0, index=self.symbols)\n",
    "        \n",
    "        # Top 4 by support proximity get positive weights\n",
    "        for symbol in top_4_support:\n",
    "            weights[symbol] = 1/4\n",
    "            \n",
    "        # Top 4 by resistance proximity get negative weights\n",
    "        for symbol in top_4_resistance:\n",
    "            weights[symbol] = -1/4\n",
    "        \n",
    "        return weights\n",
    "    \n",
    "    def task1_Strategy5(self, current_date_idx):\n",
    "        \"\"\"\n",
    "        Strategy 5: Stochastic %K\n",
    "        - Calculate %K = 100 * (Close - 14day_Low) / (14day_High - 14day_Low)\n",
    "        - Top 3 %K get negative weights, bottom 3 get positive weights\n",
    "        \"\"\"\n",
    "        if current_date_idx < 14:\n",
    "            return pd.Series(0, index=self.symbols)\n",
    "        \n",
    "        k_values = {}\n",
    "        \n",
    "        for symbol in self.symbols:\n",
    "            highs = self.high_prices.iloc[max(0, current_date_idx-14):current_date_idx][symbol].dropna()\n",
    "            lows = self.low_prices.iloc[max(0, current_date_idx-14):current_date_idx][symbol].dropna()\n",
    "            current_close = self.close_prices.iloc[current_date_idx-1][symbol]\n",
    "            \n",
    "            if len(highs) < 14 or len(lows) < 14 or pd.isna(current_close):\n",
    "                k_values[symbol] = 50  # Neutral value\n",
    "                continue\n",
    "            \n",
    "            high_14 = highs.max()\n",
    "            low_14 = lows.min()\n",
    "            \n",
    "            if high_14 != low_14:\n",
    "                k_percent = 100 * (current_close - low_14) / (high_14 - low_14)\n",
    "                k_values[symbol] = k_percent\n",
    "            else:\n",
    "                k_values[symbol] = 50  # Neutral value\n",
    "        \n",
    "        # Rank and assign weights\n",
    "        k_series = pd.Series(k_values)\n",
    "        ranked_stocks = k_series.sort_values(ascending=False)\n",
    "        \n",
    "        weights = pd.Series(0.0, index=self.symbols)\n",
    "        \n",
    "        # Top 3 %K get negative weights\n",
    "        top_3 = ranked_stocks.head(3).index\n",
    "        weights[top_3] = -1/3\n",
    "        \n",
    "        # Bottom 3 %K get positive weights\n",
    "        bottom_3 = ranked_stocks.tail(3).index\n",
    "        weights[bottom_3] = 1/3\n",
    "        \n",
    "        return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategyBacktester:\n",
    "    \"\"\"\n",
    "    Backtesting engine to evaluate strategy performance\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, strategies):\n",
    "        self.data = data\n",
    "        self.strategies = strategies\n",
    "        self.results = {}\n",
    "        \n",
    "    def calculate_returns(self, weights_df):\n",
    "        \"\"\"Calculate returns for given weights\"\"\"\n",
    "        close_prices = self.strategies.close_prices\n",
    "        \n",
    "        # Calculate daily returns\n",
    "        price_returns = close_prices.pct_change().fillna(0)\n",
    "        \n",
    "        # Calculate portfolio returns\n",
    "        portfolio_returns = []\n",
    "        \n",
    "        for date in weights_df.index:\n",
    "            if date in price_returns.index:\n",
    "                weights = weights_df.loc[date]\n",
    "                returns = price_returns.loc[date]\n",
    "                \n",
    "                # Portfolio return = sum of (weight * stock_return)\n",
    "                portfolio_return = (weights * returns).sum()\n",
    "                portfolio_returns.append(portfolio_return)\n",
    "            else:\n",
    "                portfolio_returns.append(0)\n",
    "        \n",
    "        return pd.Series(portfolio_returns, index=weights_df.index)\n",
    "    \n",
    "    def calculate_performance_metrics(self, returns):\n",
    "        \"\"\"Calculate performance metrics\"\"\"\n",
    "        total_return = (1 + returns).prod() - 1\n",
    "        annualized_return = (1 + returns.mean())**252 - 1\n",
    "        volatility = returns.std() * np.sqrt(252)\n",
    "        sharpe_ratio = annualized_return / volatility if volatility != 0 else 0\n",
    "        \n",
    "        # Maximum drawdown\n",
    "        cumulative = (1 + returns).cumprod()\n",
    "        peak = cumulative.expanding().max()\n",
    "        drawdown = (peak - cumulative) / peak\n",
    "        max_drawdown = drawdown.max()\n",
    "        \n",
    "        return {\n",
    "            'total_return': total_return,\n",
    "            'annualized_return': annualized_return,\n",
    "            'volatility': volatility,\n",
    "            'sharpe_ratio': sharpe_ratio,\n",
    "            'max_drawdown': max_drawdown\n",
    "        }\n",
    "    \n",
    "    def backtest_strategy(self, strategy_name, start_idx=250, end_idx=3500):\n",
    "        \"\"\"Backtest a single strategy\"\"\"\n",
    "        weights_list = []\n",
    "        dates_list = []\n",
    "        \n",
    "        strategy_func = getattr(self.strategies, f'task1_{strategy_name}')\n",
    "        \n",
    "        for i in range(start_idx, min(end_idx, len(self.strategies.dates))):\n",
    "            date = self.strategies.dates[i]\n",
    "            weights = strategy_func(i)\n",
    "            \n",
    "            weights_list.append(weights.values)\n",
    "            dates_list.append(date)\n",
    "        \n",
    "        weights_df = pd.DataFrame(weights_list, \n",
    "                                 index=dates_list, \n",
    "                                 columns=self.strategies.symbols)\n",
    "        \n",
    "        returns = self.calculate_returns(weights_df)\n",
    "        performance = self.calculate_performance_metrics(returns)\n",
    "        \n",
    "        return weights_df, returns, performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleStrategySelector:\n",
    "    \"\"\"\n",
    "    Ensemble strategy selection using clustering approach\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, strategies, backtester):\n",
    "        self.strategies = strategies\n",
    "        self.backtester = backtester\n",
    "        self.strategy_names = ['Strategy1', 'Strategy2', 'Strategy3', 'Strategy4', 'Strategy5']\n",
    "        self.model = None\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def extract_market_features(self, current_date_idx, lookback=20):\n",
    "        \"\"\"Extract market features for clustering\"\"\"\n",
    "        if current_date_idx < lookback:\n",
    "            return np.zeros(10)  # Return neutral features\n",
    "        \n",
    "        # Get recent market data\n",
    "        recent_closes = self.strategies.close_prices.iloc[current_date_idx-lookback:current_date_idx]\n",
    "        recent_volumes = self.strategies.volume_data.iloc[current_date_idx-lookback:current_date_idx]\n",
    "        \n",
    "        features = []\n",
    "        \n",
    "        # Market volatility\n",
    "        market_returns = recent_closes.pct_change().fillna(0)\n",
    "        avg_volatility = market_returns.std().mean()\n",
    "        features.append(avg_volatility)\n",
    "        \n",
    "        # Market trend\n",
    "        market_trend = (recent_closes.iloc[-1] / recent_closes.iloc[0] - 1).mean()\n",
    "        features.append(market_trend)\n",
    "        \n",
    "        # Volume trend\n",
    "        volume_trend = (recent_volumes.iloc[-1] / recent_volumes.iloc[0] - 1).mean()\n",
    "        features.append(volume_trend)\n",
    "        \n",
    "        # Cross-sectional dispersion\n",
    "        latest_returns = market_returns.iloc[-1]\n",
    "        dispersion = latest_returns.std()\n",
    "        features.append(dispersion)\n",
    "        \n",
    "        # Momentum (5-day vs 20-day)\n",
    "        short_ma = recent_closes.tail(5).mean().mean()\n",
    "        long_ma = recent_closes.mean().mean()\n",
    "        momentum = (short_ma / long_ma - 1) if long_ma != 0 else 0\n",
    "        features.append(momentum)\n",
    "        \n",
    "        # Additional technical features\n",
    "        # RSI-like momentum\n",
    "        gains = market_returns[market_returns > 0].sum().sum()\n",
    "        losses = abs(market_returns[market_returns < 0].sum().sum())\n",
    "        rsi = gains / (gains + losses) if (gains + losses) != 0 else 0.5\n",
    "        features.append(rsi)\n",
    "        \n",
    "        # VIX-like fear index (volatility of volatility)\n",
    "        rolling_vol = market_returns.rolling(5).std()\n",
    "        vol_of_vol = rolling_vol.std().mean()\n",
    "        features.append(vol_of_vol)\n",
    "        \n",
    "        # Market correlation\n",
    "        corr_matrix = market_returns.corr()\n",
    "        avg_correlation = corr_matrix.mean().mean()\n",
    "        features.append(avg_correlation)\n",
    "        \n",
    "        # Skewness and Kurtosis\n",
    "        skewness = market_returns.skew().mean()\n",
    "        kurtosis = market_returns.kurtosis().mean()\n",
    "        features.extend([skewness, kurtosis])\n",
    "        \n",
    "        return np.array(features)\n",
    "    \n",
    "    def train_ensemble_selector(self, train_start=250, train_end=3500, validation_window=50):\n",
    "        \"\"\"\n",
    "        Train the ensemble selector using clustering approach\n",
    "        \"\"\"\n",
    "        print(\"Training ensemble strategy selector...\")\n",
    "        \n",
    "        # Step 1: Evaluate all strategies on rolling windows\n",
    "        strategy_performances = []\n",
    "        market_features_list = []\n",
    "        best_strategies = []\n",
    "        \n",
    "        for window_start in range(train_start, train_end - validation_window, validation_window):\n",
    "            window_end = window_start + validation_window\n",
    "            \n",
    "            # Extract market features for this period\n",
    "            features = self.extract_market_features(window_start)\n",
    "            market_features_list.append(features)\n",
    "            \n",
    "            # Evaluate all strategies in this window\n",
    "            window_performances = []\n",
    "            \n",
    "            for strategy_name in self.strategy_names:\n",
    "                _, returns, performance = self.backtester.backtest_strategy(\n",
    "                    strategy_name, window_start, window_end\n",
    "                )\n",
    "                \n",
    "                # Focus on Sharpe ratio as primary metric\n",
    "                sharpe = performance['sharpe_ratio']\n",
    "                total_ret = performance['total_return']\n",
    "                max_dd = performance['max_drawdown']\n",
    "                \n",
    "                # Combined score (you can adjust weights)\n",
    "                combined_score = sharpe * 0.6 + total_ret * 0.3 - abs(max_dd) * 0.1\n",
    "                window_performances.append(combined_score)\n",
    "            \n",
    "            strategy_performances.append(window_performances)\n",
    "            \n",
    "            # Find best strategy for this period\n",
    "            best_strategy_idx = np.argmax(window_performances)\n",
    "            best_strategies.append(best_strategy_idx)\n",
    "        \n",
    "        # Step 2: Create feature matrix\n",
    "        X = np.array(market_features_list)\n",
    "        y = np.array(best_strategies)\n",
    "        \n",
    "        # Scale features\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # Step 3: Cluster market conditions and map to best strategies\n",
    "        n_clusters = 5  # One for each strategy\n",
    "        \n",
    "        self.model = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        clusters = self.model.fit_predict(X_scaled)\n",
    "        \n",
    "        # Map each cluster to the most successful strategy\n",
    "        self.cluster_strategy_map = {}\n",
    "        \n",
    "        for cluster_id in range(n_clusters):\n",
    "            cluster_mask = clusters == cluster_id\n",
    "            if cluster_mask.sum() > 0:\n",
    "                cluster_strategies = np.array(best_strategies)[cluster_mask]\n",
    "                # Most common strategy in this cluster\n",
    "                most_common_strategy = np.bincount(cluster_strategies).argmax()\n",
    "                self.cluster_strategy_map[cluster_id] = most_common_strategy\n",
    "            else:\n",
    "                self.cluster_strategy_map[cluster_id] = 0  # Default to Strategy1\n",
    "        \n",
    "        print(f\"Training completed. Cluster-Strategy mapping: {self.cluster_strategy_map}\")\n",
    "        \n",
    "        return X_scaled, clusters, best_strategies\n",
    "    \n",
    "    def predict_best_strategy(self, current_date_idx):\n",
    "        \"\"\"Predict best strategy for current market conditions\"\"\"\n",
    "        if self.model is None:\n",
    "            return 0  # Default to Strategy1\n",
    "        \n",
    "        # Extract current market features\n",
    "        features = self.extract_market_features(current_date_idx)\n",
    "        features_scaled = self.scaler.transform(features.reshape(1, -1))\n",
    "        \n",
    "        # Predict cluster\n",
    "        cluster = self.model.predict(features_scaled)[0]\n",
    "        \n",
    "        # Map to strategy\n",
    "        strategy_idx = self.cluster_strategy_map.get(cluster, 0)\n",
    "        \n",
    "        return strategy_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task1(train_data_path='train_data.csv', test_data_path='test_data.csv'):\n",
    "    \"\"\"\n",
    "    Task 1: Implement and test all 5 strategies on both train and test data\n",
    "    \"\"\"\n",
    "    print(\"=== TASK 1: Individual Strategy Implementation ===\")\n",
    "\n",
    "    # Load training data for strategy development\n",
    "    train_data = pd.read_csv(train_data_path)\n",
    "    train_data = train_data.iloc[:, 1:]\n",
    "    \n",
    "    print(f\"Training data shape: {train_data.shape}\")\n",
    "    \n",
    "    # Load test data for backtesting\n",
    "    test_data = pd.read_csv(test_data_path)\n",
    "    test_data = test_data.iloc[:, 1:]\n",
    "    \n",
    "    print(f\"Test data shape: {test_data.shape}\")\n",
    "    \n",
    "    # Initialize strategies with test data for backtesting\n",
    "    strategies = TradingStrategies(test_data)\n",
    "    strategies.prepare_data()\n",
    "    \n",
    "    backtester = StrategyBacktester(test_data, strategies)\n",
    "    \n",
    "    # Test all strategies on test data\n",
    "    results = {}\n",
    "    for strategy_name in ['Strategy1', 'Strategy2', 'Strategy3', 'Strategy4', 'Strategy5']:\n",
    "        print(f\"\\nBacktesting {strategy_name} on test data...\")\n",
    "        weights_df, returns, performance = backtester.backtest_strategy(strategy_name)\n",
    "        results[strategy_name] = {\n",
    "            'weights': weights_df,\n",
    "            'returns': returns,\n",
    "            'performance': performance\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{strategy_name} Performance on Test Data:\")\n",
    "        for metric, value in performance.items():\n",
    "            print(f\"  {metric}: {value:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task2(train_data_path='train_data.csv', test_data_path='test_data.csv'):\n",
    "    \"\"\"\n",
    "    Task 2: Train ensemble on training data, backtest on test data\n",
    "    \"\"\"\n",
    "    print(\"=== TASK 2: Ensemble Strategy Selection ===\")\n",
    "    \n",
    "    # Load training data\n",
    "    train_data = pd.read_csv(train_data_path)\n",
    "    train_data = train_data.iloc[:, 1:]\n",
    "    \n",
    "    print(f\"Training data shape: {train_data.shape}\")\n",
    "    \n",
    "    # Load test data\n",
    "    test_data = pd.read_csv(test_data_path)\n",
    "    test_data = test_data.iloc[:, 1:]\n",
    "    \n",
    "    print(f\"Test data shape: {test_data.shape}\")\n",
    "    \n",
    "    # TRAINING PHASE: Train ensemble selector on training data\n",
    "    print(\"\\n--- TRAINING PHASE ---\")\n",
    "    train_strategies = TradingStrategies(train_data)\n",
    "    train_strategies.prepare_data()\n",
    "    \n",
    "    train_backtester = StrategyBacktester(train_data, train_strategies)\n",
    "    ensemble_selector = EnsembleStrategySelector(train_strategies, train_backtester)\n",
    "    \n",
    "    # Train ensemble selector on training data only\n",
    "    print(\"Training ensemble selector on training data...\")\n",
    "    X_scaled, clusters, best_strategies = ensemble_selector.train_ensemble_selector(\n",
    "        train_start=250, \n",
    "        train_end=len(train_strategies.dates),  # Use all available training data\n",
    "        validation_window=50\n",
    "    )\n",
    "    \n",
    "    print(f\"Training completed. Cluster-Strategy mapping: {ensemble_selector.cluster_strategy_map}\")\n",
    "    \n",
    "    # TESTING PHASE: Apply trained model to test data\n",
    "    print(\"\\n--- TESTING PHASE ---\")\n",
    "    test_strategies = TradingStrategies(test_data)\n",
    "    test_strategies.prepare_data()\n",
    "    \n",
    "    test_backtester = StrategyBacktester(test_data, test_strategies)\n",
    "    \n",
    "    # Apply trained ensemble selector to test data\n",
    "    # We need to transfer the trained model to work with test data\n",
    "    test_ensemble_selector = EnsembleStrategySelector(test_strategies, test_backtester)\n",
    "    test_ensemble_selector.model = ensemble_selector.model\n",
    "    test_ensemble_selector.scaler = ensemble_selector.scaler\n",
    "    test_ensemble_selector.cluster_strategy_map = ensemble_selector.cluster_strategy_map\n",
    "    test_ensemble_selector.strategy_names = ensemble_selector.strategy_names\n",
    "    \n",
    "    # Generate ensemble weights for test period\n",
    "    ensemble_weights = []\n",
    "    ensemble_dates = []\n",
    "    strategy_selections = []\n",
    "    \n",
    "    print(\"Generating predictions on test data...\")\n",
    "    \n",
    "    test_start = 250 if len(test_strategies.dates) > 250 else 50  # Adjust if test data is shorter\n",
    "    \n",
    "    for i in range(test_start, len(test_strategies.dates)):\n",
    "        date = test_strategies.dates[i]\n",
    "        \n",
    "        # Predict best strategy using trained model\n",
    "        best_strategy_idx = test_ensemble_selector.predict_best_strategy(i)\n",
    "        strategy_name = test_ensemble_selector.strategy_names[best_strategy_idx]\n",
    "        \n",
    "        # Get weights from selected strategy\n",
    "        strategy_func = getattr(test_strategies, f'task1_{strategy_name}')\n",
    "        weights = strategy_func(i)\n",
    "        \n",
    "        ensemble_weights.append(weights.values)\n",
    "        ensemble_dates.append(date)\n",
    "        strategy_selections.append(strategy_name)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f\"  Processed {i-test_start}/{len(test_strategies.dates)-test_start} test days\")\n",
    "    \n",
    "    # Create weights DataFrame\n",
    "    ensemble_weights_df = pd.DataFrame(\n",
    "        ensemble_weights, \n",
    "        index=ensemble_dates, \n",
    "        columns=test_strategies.symbols\n",
    "    )\n",
    "    \n",
    "    # Calculate ensemble performance on test data\n",
    "    ensemble_returns = test_backtester.calculate_returns(ensemble_weights_df)\n",
    "    ensemble_performance = test_backtester.calculate_performance_metrics(ensemble_returns)\n",
    "    \n",
    "    # Save ensemble_performance as CSV\n",
    "    ensemble_performance_df = pd.DataFrame([ensemble_performance])\n",
    "    ensemble_performance_df.to_csv('backtest_performance_metrics.csv', index=False)\n",
    "    \n",
    "    print(\"\\nEnsemble Strategy Performance on Test Data:\")\n",
    "    for metric, value in ensemble_performance.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    \n",
    "    print(f\"\\nStrategy Selection Distribution on Test Data:\")\n",
    "    selection_counts = pd.Series(strategy_selections).value_counts()\n",
    "    for strategy, count in selection_counts.items():\n",
    "        pct = count / len(strategy_selections) * 100\n",
    "        print(f\"  {strategy}: {count} days ({pct:.1f}%)\")\n",
    "    \n",
    "    # Save outputs\n",
    "    ensemble_weights_df.to_csv('ensemble_weights.csv')\n",
    "    \n",
    "    # Save trained model (this is the model trained on training data)\n",
    "    model_data = {\n",
    "        'model': ensemble_selector.model,\n",
    "        'scaler': ensemble_selector.scaler,\n",
    "        'cluster_strategy_map': ensemble_selector.cluster_strategy_map,\n",
    "        'strategy_names': ensemble_selector.strategy_names,\n",
    "        'training_data_info': {\n",
    "            'train_data_shape': train_data.shape,\n",
    "            'test_data_shape': test_data.shape,\n",
    "            'train_period': (250, len(train_strategies.dates)),\n",
    "            'test_period': (test_start, len(test_strategies.dates))\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open('ensemble_model.pkl', 'wb') as f:\n",
    "        pickle.dump(model_data, f)\n",
    "    \n",
    "    # Performance CSV (test data results)\n",
    "    performance_df = pd.DataFrame({\n",
    "        'Date': ensemble_dates,\n",
    "        'Returns': ensemble_returns.values,\n",
    "        'Cumulative_Returns': (1 + ensemble_returns).cumprod().values,\n",
    "        'Selected_Strategy': strategy_selections\n",
    "    })\n",
    "    performance_df.to_csv('ensemble_performance.csv', index=False)\n",
    "    \n",
    "    return ensemble_weights_df, ensemble_performance, model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_strategy_comparison(results):\n",
    "    \"\"\"Plot comparison of strategy performances\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    strategies = list(results.keys())\n",
    "    metrics = ['sharpe_ratio', 'total_return', 'volatility', 'max_drawdown']\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        ax = axes[i//2, i%2]\n",
    "        values = [results[strategy]['performance'][metric] for strategy in strategies]\n",
    "        ax.bar(strategies, values)\n",
    "        ax.set_title(f'{metric.replace(\"_\", \" \").title()}')\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_ensemble_selections(strategy_selections):\n",
    "    \"\"\"Analyze which strategies were selected when\"\"\"\n",
    "    selection_counts = pd.Series(strategy_selections).value_counts()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    selection_counts.plot(kind='bar')\n",
    "    plt.title('Strategy Selection Frequency')\n",
    "    plt.ylabel('Number of Days Selected')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "    \n",
    "    return selection_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING COMPLETE TRADING PIPELINE\n",
      "==================================================\n",
      "\n",
      "1. Running individual strategies on test data...\n",
      "=== TASK 1: Individual Strategy Implementation ===\n",
      "Training data shape: (70000, 14)\n",
      "Test data shape: (11120, 14)\n",
      "\n",
      "Backtesting Strategy1 on test data...\n",
      "\n",
      "Strategy1 Performance on Test Data:\n",
      "  total_return: 0.4139\n",
      "  annualized_return: 0.3803\n",
      "  volatility: 0.2763\n",
      "  sharpe_ratio: 1.3761\n",
      "  max_drawdown: 0.1682\n",
      "\n",
      "Backtesting Strategy2 on test data...\n",
      "\n",
      "Strategy2 Performance on Test Data:\n",
      "  total_return: -0.0832\n",
      "  annualized_return: -0.0315\n",
      "  volatility: 0.2811\n",
      "  sharpe_ratio: -0.1122\n",
      "  max_drawdown: 0.2353\n",
      "\n",
      "Backtesting Strategy3 on test data...\n",
      "\n",
      "Strategy3 Performance on Test Data:\n",
      "  total_return: 0.0526\n",
      "  annualized_return: 0.0825\n",
      "  volatility: 0.2730\n",
      "  sharpe_ratio: 0.3021\n",
      "  max_drawdown: 0.2776\n",
      "\n",
      "Backtesting Strategy4 on test data...\n",
      "\n",
      "Strategy4 Performance on Test Data:\n",
      "  total_return: -0.0458\n",
      "  annualized_return: -0.0073\n",
      "  volatility: 0.2500\n",
      "  sharpe_ratio: -0.0293\n",
      "  max_drawdown: 0.1940\n",
      "\n",
      "Backtesting Strategy5 on test data...\n",
      "\n",
      "Strategy5 Performance on Test Data:\n",
      "  total_return: 0.0817\n",
      "  annualized_return: 0.1226\n",
      "  volatility: 0.3192\n",
      "  sharpe_ratio: 0.3840\n",
      "  max_drawdown: 0.2679\n",
      "\n",
      "2. Training ensemble on training data and testing on test data...\n",
      "=== TASK 2: Ensemble Strategy Selection ===\n",
      "Training data shape: (70000, 14)\n",
      "Test data shape: (11120, 14)\n",
      "\n",
      "--- TRAINING PHASE ---\n",
      "Training ensemble selector on training data...\n",
      "Training ensemble strategy selector...\n",
      "Training completed. Cluster-Strategy mapping: {0: np.int64(4), 1: np.int64(1), 2: np.int64(4), 3: np.int64(4), 4: np.int64(2)}\n",
      "Training completed. Cluster-Strategy mapping: {0: np.int64(4), 1: np.int64(1), 2: np.int64(4), 3: np.int64(4), 4: np.int64(2)}\n",
      "\n",
      "--- TESTING PHASE ---\n",
      "Generating predictions on test data...\n",
      "  Processed 50/306 test days\n",
      "  Processed 150/306 test days\n",
      "  Processed 250/306 test days\n",
      "\n",
      "Ensemble Strategy Performance on Test Data:\n",
      "  total_return: -0.0887\n",
      "  annualized_return: -0.0336\n",
      "  volatility: 0.2912\n",
      "  sharpe_ratio: -0.1154\n",
      "  max_drawdown: 0.3593\n",
      "\n",
      "Strategy Selection Distribution on Test Data:\n",
      "  Strategy2: 174 days (56.9%)\n",
      "  Strategy3: 67 days (21.9%)\n",
      "  Strategy5: 65 days (21.2%)\n",
      "\n",
      "==================================================\n",
      "PIPELINE COMPLETED SUCCESSFULLY\n",
      "==================================================\n",
      "\n",
      "Generated Files:\n",
      "1. ensemble_weights.csv - Portfolio weights from test data\n",
      "2. ensemble_performance.csv - Performance metrics from test data\n",
      "3. backtest_performance_metrics.csv - Summary performance metrics\n",
      "4. ensemble_model.pkl - Trained model (trained on training data)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def run_complete_pipeline(train_data_path='train_data.csv', test_data_path='test_data.csv'):\n",
    "    \"\"\"\n",
    "    Run complete pipeline: train on training data, test on test data\n",
    "    \"\"\"\n",
    "    print(\"RUNNING COMPLETE TRADING PIPELINE\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Task 1: Individual strategies on test data\n",
    "    print(\"\\n1. Running individual strategies on test data...\")\n",
    "    task1_results = task1(train_data_path, test_data_path)\n",
    "    \n",
    "    # Task 2: Train ensemble on training data, test on test data\n",
    "    print(\"\\n2. Training ensemble on training data and testing on test data...\")\n",
    "    ensemble_weights_df, ensemble_performance, model_data = task2(train_data_path, test_data_path)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    print(\"\\nGenerated Files:\")\n",
    "    print(\"1. ensemble_weights.csv - Portfolio weights from test data\")\n",
    "    print(\"2. ensemble_performance.csv - Performance metrics from test data\") \n",
    "    print(\"3. backtest_performance_metrics.csv - Summary performance metrics\")\n",
    "    print(\"4. ensemble_model.pkl - Trained model (trained on training data)\")\n",
    "    \n",
    "    return task1_results, ensemble_weights_df, ensemble_performance, model_data\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    task1_results, ensemble_weights, ensemble_perf, model = run_complete_pipeline(\n",
    "        train_data_path='train_data.csv',\n",
    "        test_data_path='test_data.csv'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q hmmlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hmmlearn import hmm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class HMMStrategySelector:\n",
    "    \"\"\"\n",
    "    Hidden Markov Model for trading strategy selection\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_regimes=3, covariance_type='full'):\n",
    "        self.n_regimes = n_regimes\n",
    "        self.model = hmm.GaussianHMM(\n",
    "            n_components=n_regimes, \n",
    "            covariance_type=covariance_type,\n",
    "            n_iter=100,\n",
    "            random_state=42\n",
    "        )\n",
    "        self.scaler = StandardScaler()\n",
    "        self.regime_strategy_map = {}\n",
    "        self.strategy_names = ['Strategy1', 'Strategy2', 'Strategy3', 'Strategy4', 'Strategy5']\n",
    "        self.regime_names = {\n",
    "            0: \"Bull Market\",\n",
    "            1: \"Bear Market\", \n",
    "            2: \"Volatile/Sideways\"\n",
    "        }\n",
    "        self.is_trained = False\n",
    "        \n",
    "    def extract_regime_features(self, strategies, current_idx, lookback=20):\n",
    "        \"\"\"\n",
    "        Extract features that help identify market regimes\n",
    "        \"\"\"\n",
    "        if current_idx < lookback:\n",
    "            return np.zeros(8)  # Return neutral features for early indices\n",
    "        \n",
    "        # Get recent price data\n",
    "        recent_prices = strategies.close_prices.iloc[current_idx-lookback:current_idx]\n",
    "        \n",
    "        features = []\n",
    "        \n",
    "        # 1. Average returns\n",
    "        returns = recent_prices.pct_change().fillna(0)\n",
    "        avg_return = returns.mean().mean()\n",
    "        features.append(avg_return)\n",
    "        \n",
    "        # 2. Volatility (key regime indicator)\n",
    "        volatility = returns.std().mean()\n",
    "        features.append(volatility)\n",
    "        \n",
    "        # 3. Trend strength (directional persistence)\n",
    "        trend_strength = (recent_prices.iloc[-1] / recent_prices.iloc[0] - 1).mean()\n",
    "        features.append(trend_strength)\n",
    "        \n",
    "        # 4. Market correlation (risk-on vs risk-off)\n",
    "        correlation_matrix = returns.corr().fillna(0)\n",
    "        avg_correlation = correlation_matrix.values[np.triu_indices_from(correlation_matrix.values, k=1)].mean()\n",
    "        features.append(avg_correlation)\n",
    "        \n",
    "        # 5. Skewness (tail risk indicator)\n",
    "        skewness = returns.skew().mean()\n",
    "        if np.isnan(skewness):\n",
    "            skewness = 0\n",
    "        features.append(skewness)\n",
    "        \n",
    "        # 6. Kurtosis (extreme moves indicator)  \n",
    "        kurtosis = returns.kurtosis().mean()\n",
    "        if np.isnan(kurtosis):\n",
    "            kurtosis = 0\n",
    "        features.append(kurtosis)\n",
    "        \n",
    "        # 7. Volume trend (using price as proxy if volume not available)\n",
    "        volume_proxy = recent_prices.sum(axis=1)  # Sum of all prices as volume proxy\n",
    "        volume_trend = (volume_proxy.iloc[-1] / volume_proxy.iloc[0] - 1) if volume_proxy.iloc[0] != 0 else 0\n",
    "        features.append(volume_trend)\n",
    "        \n",
    "        # 8. Volatility of volatility\n",
    "        rolling_vol = returns.rolling(5).std()\n",
    "        vol_of_vol = rolling_vol.std().mean()\n",
    "        if np.isnan(vol_of_vol):\n",
    "            vol_of_vol = 0\n",
    "        features.append(vol_of_vol)\n",
    "        \n",
    "        return np.array(features)\n",
    "    \n",
    "    def train_hmm_selector(self, strategies, backtester, train_start=250, train_end=None, window_size=50):\n",
    "        \"\"\"\n",
    "        Train HMM to detect market regimes and map them to best strategies\n",
    "        \"\"\"\n",
    "        print(\"Training HMM regime detector...\")\n",
    "        \n",
    "        if train_end is None:\n",
    "            train_end = len(strategies.dates) - window_size\n",
    "        \n",
    "        # Step 1: Extract features and evaluate strategies over rolling windows\n",
    "        feature_matrix = []\n",
    "        best_strategy_sequence = []\n",
    "        regime_performance_history = []\n",
    "        \n",
    "        for window_start in range(train_start, train_end, window_size):\n",
    "            window_end = min(window_start + window_size, len(strategies.dates))\n",
    "            \n",
    "            if window_end - window_start < 20:  # Skip if window too small\n",
    "                continue\n",
    "                \n",
    "            # Extract regime features for this window (use midpoint)\n",
    "            feature_idx = window_start + (window_end - window_start) // 2\n",
    "            features = self.extract_regime_features(strategies, feature_idx)\n",
    "            feature_matrix.append(features)\n",
    "            \n",
    "            # Evaluate all strategies in this window\n",
    "            strategy_performances = {}\n",
    "            \n",
    "            for i, strategy_name in enumerate(self.strategy_names):\n",
    "                try:\n",
    "                    # Backtest strategy for this window\n",
    "                    strategy_weights = []\n",
    "                    strategy_dates = []\n",
    "                    \n",
    "                    for j in range(window_start, window_end):\n",
    "                        if j < len(strategies.dates):\n",
    "                            strategy_func = getattr(strategies, f'task1_{strategy_name}')\n",
    "                            weights = strategy_func(j)\n",
    "                            strategy_weights.append(weights.values)\n",
    "                            strategy_dates.append(strategies.dates[j])\n",
    "                    \n",
    "                    if len(strategy_weights) > 0:\n",
    "                        weights_df = pd.DataFrame(\n",
    "                            strategy_weights,\n",
    "                            index=strategy_dates,\n",
    "                            columns=strategies.symbols\n",
    "                        )\n",
    "                        \n",
    "                        # Calculate returns and performance\n",
    "                        returns = backtester.calculate_returns(weights_df)\n",
    "                        performance = backtester.calculate_performance_metrics(returns)\n",
    "                        \n",
    "                        # Combined performance score\n",
    "                        sharpe = performance.get('sharpe_ratio', 0)\n",
    "                        total_ret = performance.get('total_return', 0)\n",
    "                        max_dd = performance.get('max_drawdown', 0)\n",
    "                        \n",
    "                        # Weighted score (adjust weights as needed)\n",
    "                        score = sharpe * 0.5 + total_ret * 0.3 - abs(max_dd) * 0.2\n",
    "                        strategy_performances[i] = score\n",
    "                    else:\n",
    "                        strategy_performances[i] = 0\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"Error evaluating {strategy_name} in window {window_start}-{window_end}: {e}\")\n",
    "                    strategy_performances[i] = 0\n",
    "            \n",
    "            # Find best strategy for this window\n",
    "            if strategy_performances:\n",
    "                best_strategy_idx = max(strategy_performances, key=strategy_performances.get)\n",
    "                best_strategy_sequence.append(best_strategy_idx)\n",
    "                regime_performance_history.append(strategy_performances)\n",
    "            \n",
    "        if len(feature_matrix) == 0:\n",
    "            raise ValueError(\"No valid training windows found. Check data length and parameters.\")\n",
    "        \n",
    "        # Step 2: Scale features\n",
    "        feature_matrix = np.array(feature_matrix)\n",
    "        \n",
    "        # Handle NaN/Inf values\n",
    "        feature_matrix = np.nan_to_num(feature_matrix, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "        \n",
    "        feature_matrix_scaled = self.scaler.fit_transform(feature_matrix)\n",
    "        \n",
    "        # Step 3: Fit HMM to identify regimes\n",
    "        print(f\"Fitting HMM with {self.n_regimes} regimes on {len(feature_matrix)} windows...\")\n",
    "        self.model.fit(feature_matrix_scaled)\n",
    "        \n",
    "        # Step 4: Predict regimes for training data\n",
    "        predicted_regimes = self.model.predict(feature_matrix_scaled)\n",
    "        \n",
    "        # Step 5: Map each regime to its most successful strategy\n",
    "        for regime_id in range(self.n_regimes):\n",
    "            regime_mask = predicted_regimes == regime_id\n",
    "            \n",
    "            if regime_mask.sum() > 0:\n",
    "                # Get strategies used in this regime\n",
    "                regime_strategies = np.array(best_strategy_sequence)[regime_mask]\n",
    "                \n",
    "                # Find most common strategy in this regime\n",
    "                strategy_counts = np.bincount(regime_strategies, minlength=len(self.strategy_names))\n",
    "                most_common_strategy = np.argmax(strategy_counts)\n",
    "                \n",
    "                self.regime_strategy_map[regime_id] = most_common_strategy\n",
    "                \n",
    "                # Calculate regime statistics\n",
    "                regime_performances = np.array(regime_performance_history)[regime_mask]\n",
    "                avg_performance = np.mean([perf[most_common_strategy] for perf in regime_performances])\n",
    "                \n",
    "                print(f\"Regime {regime_id} ({self.regime_names.get(regime_id, f'Regime_{regime_id}')}):\")\n",
    "                print(f\"  Best Strategy: {self.strategy_names[most_common_strategy]}\")\n",
    "                print(f\"  Occurrences: {regime_mask.sum()} windows ({regime_mask.sum()/len(predicted_regimes)*100:.1f}%)\")\n",
    "                print(f\"  Avg Performance: {avg_performance:.4f}\")\n",
    "            else:\n",
    "                # Default mapping if regime never occurs\n",
    "                self.regime_strategy_map[regime_id] = 0\n",
    "                print(f\"Regime {regime_id}: No occurrences, defaulting to {self.strategy_names[0]}\")\n",
    "        \n",
    "        self.is_trained = True\n",
    "        \n",
    "        return feature_matrix_scaled, predicted_regimes, best_strategy_sequence\n",
    "    \n",
    "    def predict_current_regime(self, strategies, current_idx):\n",
    "        \"\"\"\n",
    "        Predict current market regime\n",
    "        \"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model must be trained first!\")\n",
    "        \n",
    "        # Extract current features\n",
    "        current_features = self.extract_regime_features(strategies, current_idx)\n",
    "        \n",
    "        # Handle NaN/Inf values\n",
    "        current_features = np.nan_to_num(current_features, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "        \n",
    "        # Scale features\n",
    "        current_features_scaled = self.scaler.transform(current_features.reshape(1, -1))\n",
    "        \n",
    "        # Predict regime\n",
    "        predicted_regime = self.model.predict(current_features_scaled)[0]\n",
    "        \n",
    "        # Get regime probabilities\n",
    "        regime_probs = self.model.predict_proba(current_features_scaled)[0]\n",
    "        \n",
    "        return predicted_regime, regime_probs\n",
    "    \n",
    "    def predict_best_strategy(self, strategies, current_idx):\n",
    "        \"\"\"\n",
    "        Select best strategy based on predicted regime\n",
    "        \"\"\"\n",
    "        predicted_regime, regime_probs = self.predict_current_regime(strategies, current_idx)\n",
    "        \n",
    "        # Get mapped strategy for this regime\n",
    "        selected_strategy_idx = self.regime_strategy_map.get(predicted_regime, 0)\n",
    "        \n",
    "        return selected_strategy_idx, predicted_regime, regime_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task2(train_data_path='train_data.csv', test_data_path='test_data.csv'):\n",
    "    \"\"\"\n",
    "    Task 2: Train HMM ensemble on training data, backtest on test data\n",
    "    \"\"\"\n",
    "    print(\"=== TASK 2: HMM-Based Strategy Selection ===\")\n",
    "    \n",
    "    # Load training data\n",
    "    train_data = pd.read_csv(train_data_path)\n",
    "    train_data = train_data.iloc[:, 1:]\n",
    "    \n",
    "    print(f\"Training data shape: {train_data.shape}\")\n",
    "    \n",
    "    # Load test data\n",
    "    test_data = pd.read_csv(test_data_path)\n",
    "    test_data = test_data.iloc[:, 1:]\n",
    "    \n",
    "    print(f\"Test data shape: {test_data.shape}\")\n",
    "    \n",
    "    # TRAINING PHASE: Train HMM selector on training data\n",
    "    print(\"\\n--- TRAINING PHASE ---\")\n",
    "    train_strategies = TradingStrategies(train_data)\n",
    "    train_strategies.prepare_data()\n",
    "    \n",
    "    train_backtester = StrategyBacktester(train_data, train_strategies)\n",
    "    hmm_selector = HMMStrategySelector(n_regimes=3)\n",
    "    \n",
    "    # Train HMM selector on training data only\n",
    "    print(\"Training HMM selector on training data...\")\n",
    "    features, regimes, best_strategies = hmm_selector.train_hmm_selector(\n",
    "        train_strategies, \n",
    "        train_backtester,\n",
    "        train_start=250, \n",
    "        train_end=len(train_strategies.dates) - 50,  # Leave some buffer\n",
    "        window_size=50\n",
    "    )\n",
    "    \n",
    "    print(f\"Training completed. Regime-Strategy mapping: {hmm_selector.regime_strategy_map}\")\n",
    "    \n",
    "    # TESTING PHASE: Apply trained model to test data\n",
    "    print(\"\\n--- TESTING PHASE ---\")\n",
    "    test_strategies = TradingStrategies(test_data)\n",
    "    test_strategies.prepare_data()\n",
    "    \n",
    "    test_backtester = StrategyBacktester(test_data, test_strategies)\n",
    "    \n",
    "    # Apply trained HMM selector to test data\n",
    "    test_hmm_selector = HMMStrategySelector(n_regimes=3)\n",
    "    test_hmm_selector.model = hmm_selector.model\n",
    "    test_hmm_selector.scaler = hmm_selector.scaler\n",
    "    test_hmm_selector.regime_strategy_map = hmm_selector.regime_strategy_map\n",
    "    test_hmm_selector.strategy_names = hmm_selector.strategy_names\n",
    "    test_hmm_selector.regime_names = hmm_selector.regime_names\n",
    "    test_hmm_selector.is_trained = True\n",
    "    \n",
    "    # Generate ensemble weights for test period\n",
    "    ensemble_weights = []\n",
    "    ensemble_dates = []\n",
    "    strategy_selections = []\n",
    "    regime_predictions = []\n",
    "    regime_confidences = []\n",
    "    \n",
    "    print(\"Generating predictions on test data...\")\n",
    "    \n",
    "    # Fixed lookback requirement\n",
    "    LOOKBACK_REQUIRED = 250\n",
    "    \n",
    "    if len(test_strategies.dates) < LOOKBACK_REQUIRED:\n",
    "        raise ValueError(f\"Test data insufficient. Need at least {LOOKBACK_REQUIRED} days, got {len(test_strategies.dates)}\")\n",
    "    \n",
    "    for i in range(LOOKBACK_REQUIRED, len(test_strategies.dates)):\n",
    "        date = test_strategies.dates[i]\n",
    "        \n",
    "        # Predict best strategy using trained HMM model\n",
    "        best_strategy_idx, predicted_regime, regime_probs = test_hmm_selector.predict_best_strategy(test_strategies, i)\n",
    "        strategy_name = test_hmm_selector.strategy_names[best_strategy_idx]\n",
    "        \n",
    "        # Get weights from selected strategy\n",
    "        strategy_func = getattr(test_strategies, f'task1_{strategy_name}')\n",
    "        weights = strategy_func(i)\n",
    "        \n",
    "        ensemble_weights.append(weights.values)\n",
    "        ensemble_dates.append(date)\n",
    "        strategy_selections.append(strategy_name)\n",
    "        regime_predictions.append(predicted_regime)\n",
    "        regime_confidences.append(max(regime_probs))\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            regime_name = test_hmm_selector.regime_names.get(predicted_regime, f'Regime_{predicted_regime}')\n",
    "            print(f\"  Day {i-LOOKBACK_REQUIRED+1}/{len(test_strategies.dates)-LOOKBACK_REQUIRED}: {regime_name} -> {strategy_name} (conf: {max(regime_probs):.3f})\")\n",
    "    \n",
    "    # Create weights DataFrame\n",
    "    ensemble_weights_df = pd.DataFrame(\n",
    "        ensemble_weights, \n",
    "        index=ensemble_dates, \n",
    "        columns=test_strategies.symbols\n",
    "    )\n",
    "    \n",
    "    # Calculate ensemble performance on test data\n",
    "    ensemble_returns = test_backtester.calculate_returns(ensemble_weights_df)\n",
    "    ensemble_performance = test_backtester.calculate_performance_metrics(ensemble_returns)\n",
    "    \n",
    "    # Save ensemble_performance as CSV\n",
    "    ensemble_performance_df = pd.DataFrame([ensemble_performance])\n",
    "    ensemble_performance_df.to_csv('backtest_performance_metrics_hmm.csv', index=False)\n",
    "    \n",
    "    print(\"\\nHMM Ensemble Strategy Performance on Test Data:\")\n",
    "    for metric, value in ensemble_performance.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    \n",
    "    print(f\"\\nStrategy Selection Distribution on Test Data:\")\n",
    "    selection_counts = pd.Series(strategy_selections).value_counts()\n",
    "    for strategy, count in selection_counts.items():\n",
    "        pct = count / len(strategy_selections) * 100\n",
    "        print(f\"  {strategy}: {count} days ({pct:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nRegime Distribution on Test Data:\")\n",
    "    regime_counts = pd.Series(regime_predictions).value_counts()\n",
    "    for regime_id, count in regime_counts.items():\n",
    "        pct = count / len(regime_predictions) * 100\n",
    "        regime_name = test_hmm_selector.regime_names.get(regime_id, f'Regime_{regime_id}')\n",
    "        print(f\"  {regime_name}: {count} days ({pct:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nRegime-Strategy Mapping:\")\n",
    "    for regime_id, strategy_idx in test_hmm_selector.regime_strategy_map.items():\n",
    "        regime_name = test_hmm_selector.regime_names.get(regime_id, f'Regime_{regime_id}')\n",
    "        strategy_name = test_hmm_selector.strategy_names[strategy_idx]\n",
    "        print(f\"  {regime_name} -> {strategy_name}\")\n",
    "    \n",
    "    # Save outputs\n",
    "    ensemble_weights_df.to_csv('ensemble_weights_hmm.csv')\n",
    "    \n",
    "    # Save trained HMM model\n",
    "    model_data = {\n",
    "        'hmm_model': hmm_selector.model,\n",
    "        'scaler': hmm_selector.scaler,\n",
    "        'regime_strategy_map': hmm_selector.regime_strategy_map,\n",
    "        'strategy_names': hmm_selector.strategy_names,\n",
    "        'regime_names': hmm_selector.regime_names,\n",
    "        'n_regimes': hmm_selector.n_regimes,\n",
    "        'training_data_info': {\n",
    "            'train_data_shape': train_data.shape,\n",
    "            'test_data_shape': test_data.shape,\n",
    "            'train_period': (250, len(train_strategies.dates)),\n",
    "            'test_period': (LOOKBACK_REQUIRED, len(test_strategies.dates))\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open('ensemble_model_hmm.pkl', 'wb') as f:\n",
    "        pickle.dump(model_data, f)\n",
    "    \n",
    "    # Enhanced performance CSV with regime information\n",
    "    performance_df = pd.DataFrame({\n",
    "        'Date': ensemble_dates,\n",
    "        'Returns': ensemble_returns.values,\n",
    "        'Cumulative_Returns': (1 + ensemble_returns).cumprod().values,\n",
    "        'Selected_Strategy': strategy_selections,\n",
    "        'Predicted_Regime': regime_predictions,\n",
    "        'Regime_Confidence': regime_confidences,\n",
    "        'Regime_Name': [test_hmm_selector.regime_names.get(r, f'Regime_{r}') for r in regime_predictions]\n",
    "    })\n",
    "    performance_df.to_csv('ensemble_performance_hmm.csv', index=False)\n",
    "    \n",
    "    return ensemble_weights_df, ensemble_performance, model_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TASK 2: HMM-Based Strategy Selection ===\n",
      "Training data shape: (70000, 14)\n",
      "Test data shape: (11120, 14)\n",
      "\n",
      "--- TRAINING PHASE ---\n",
      "Training HMM selector on training data...\n",
      "Training HMM regime detector...\n",
      "Fitting HMM with 3 regimes on 64 windows...\n",
      "Regime 0 (Bull Market):\n",
      "  Best Strategy: Strategy2\n",
      "  Occurrences: 22 windows (34.4%)\n",
      "  Avg Performance: 1.8946\n",
      "Regime 1 (Bear Market):\n",
      "  Best Strategy: Strategy3\n",
      "  Occurrences: 36 windows (56.2%)\n",
      "  Avg Performance: 1.6031\n",
      "Regime 2 (Volatile/Sideways):\n",
      "  Best Strategy: Strategy2\n",
      "  Occurrences: 6 windows (9.4%)\n",
      "  Avg Performance: 3.9760\n",
      "Training completed. Regime-Strategy mapping: {0: np.int64(1), 1: np.int64(2), 2: np.int64(1)}\n",
      "\n",
      "--- TESTING PHASE ---\n",
      "Generating predictions on test data...\n",
      "  Day 51/306: Bull Market -> Strategy2 (conf: 1.000)\n",
      "  Day 151/306: Bull Market -> Strategy2 (conf: 1.000)\n",
      "  Day 251/306: Bull Market -> Strategy2 (conf: 1.000)\n",
      "\n",
      "HMM Ensemble Strategy Performance on Test Data:\n",
      "  total_return: -0.0832\n",
      "  annualized_return: -0.0315\n",
      "  volatility: 0.2811\n",
      "  sharpe_ratio: -0.1122\n",
      "  max_drawdown: 0.2353\n",
      "\n",
      "Strategy Selection Distribution on Test Data:\n",
      "  Strategy2: 306 days (100.0%)\n",
      "\n",
      "Regime Distribution on Test Data:\n",
      "  Bull Market: 306 days (100.0%)\n",
      "\n",
      "Regime-Strategy Mapping:\n",
      "  Bull Market -> Strategy2\n",
      "  Bear Market -> Strategy3\n",
      "  Volatile/Sideways -> Strategy2\n",
      "\n",
      "HMM-based ensemble strategy completed successfully!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Run HMM-based ensemble strategy\n",
    "    ensemble_weights, ensemble_performance, model_data = task2(\n",
    "        train_data_path='train_data.csv',\n",
    "        test_data_path='test_data.csv'\n",
    "    )\n",
    "    \n",
    "    print(\"\\nHMM-based ensemble strategy completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, deque\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "class MultiAgentRLSelector:\n",
    "    \"\"\"\n",
    "    Multiple RL agents compete and learn from each other\n",
    "    Each agent has a personality that influences strategy selection\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_agents=5, learning_rate=0.1, epsilon=0.1):\n",
    "        self.n_agents = n_agents\n",
    "        self.lr = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        # Strategy-Personality mappings based on analysis\n",
    "        self.personality_strategy_map = {\n",
    "            'conservative': [0],    # Strategy 1: Average Weekly Returns\n",
    "            'contrarian': [1],      # Strategy 2: Mean Reversion\n",
    "            'momentum': [2],        # Strategy 3: Rate of Change ROC\n",
    "            'adaptive': [3],        # Strategy 4: Support & Resistance\n",
    "            'aggressive': [4]       # Strategy 5: Stochastic %K\n",
    "        }\n",
    "        \n",
    "        # Each agent represents a different strategy selection philosophy\n",
    "        self.agents = {}\n",
    "        personalities = ['conservative', 'aggressive', 'momentum', 'contrarian', 'adaptive']\n",
    "        \n",
    "        for i in range(n_agents):\n",
    "            personality = personalities[i % len(personalities)]  # Ensure we have all personalities\n",
    "            self.agents[f'Agent_{i}'] = {\n",
    "                'q_table': defaultdict(lambda: defaultdict(float)),\n",
    "                'strategy_preferences': np.random.dirichlet([1]*5),\n",
    "                'wins': 0,\n",
    "                'total_games': 0,\n",
    "                'recent_rewards': deque(maxlen=20),\n",
    "                'personality': personality,\n",
    "                'preferred_strategies': self.personality_strategy_map[personality].copy()\n",
    "            }\n",
    "    \n",
    "    def generate_personality(self):\n",
    "        \"\"\"Give each agent a unique personality - now integrated into __init__\"\"\"\n",
    "        personalities = [\n",
    "            'conservative',  # Prefers low-risk strategies\n",
    "            'aggressive',    # Prefers high-return strategies\n",
    "            'momentum',      # Follows trends\n",
    "            'contrarian',    # Goes against trends\n",
    "            'adaptive'       # Changes based on market\n",
    "        ]\n",
    "        return random.choice(personalities)\n",
    "    \n",
    "    def get_market_state(self, strategies, current_idx):\n",
    "        \"\"\"Discretize market conditions into states\"\"\"\n",
    "        if current_idx < 20:\n",
    "            return \"neutral\"\n",
    "        \n",
    "        recent_data = strategies.close_prices.iloc[current_idx-20:current_idx]\n",
    "        returns = recent_data.pct_change().fillna(0)\n",
    "        \n",
    "        volatility = returns.std().mean()\n",
    "        trend = (recent_data.iloc[-1] / recent_data.iloc[0] - 1).mean()\n",
    "        \n",
    "        # Create discrete state\n",
    "        vol_state = \"high_vol\" if volatility > 0.02 else \"low_vol\"\n",
    "        trend_state = \"up\" if trend > 0.01 else \"down\" if trend < -0.01 else \"flat\"\n",
    "        \n",
    "        return f\"{vol_state}_{trend_state}\"\n",
    "    \n",
    "    def agent_select_strategy(self, agent_name, market_state):\n",
    "        \"\"\"Agent selects strategy based on Q-table and personality with improved mapping\"\"\"\n",
    "        agent = self.agents[agent_name]\n",
    "        personality = agent['personality']\n",
    "        \n",
    "        # Epsilon-greedy with personality-based strategy preference\n",
    "        if random.random() < self.epsilon:\n",
    "            # Exploration: 70% from preferred strategies, 30% random\n",
    "            if random.random() < 0.7:\n",
    "                # Choose from personality-matched strategies\n",
    "                strategy = random.choice(agent['preferred_strategies'])\n",
    "            else:\n",
    "                # Secondary strategy preferences based on personality traits\n",
    "                if personality == 'conservative':\n",
    "                    # May occasionally try other stable strategies\n",
    "                    strategy = random.choice([0, 1, 3])  # Conservative, contrarian, adaptive\n",
    "                elif personality == 'aggressive':\n",
    "                    # May try momentum or high-risk strategies\n",
    "                    strategy = random.choice([2, 4])  # Momentum, aggressive\n",
    "                elif personality == 'momentum':\n",
    "                    # May try aggressive or adaptive strategies\n",
    "                    strategy = random.choice([2, 3, 4])  # Momentum, adaptive, aggressive\n",
    "                elif personality == 'contrarian':\n",
    "                    # May try conservative or adaptive strategies\n",
    "                    strategy = random.choice([0, 1, 3])  # Conservative, contrarian, adaptive\n",
    "                else:  # adaptive\n",
    "                    # Can try any strategy based on market conditions\n",
    "                    if \"high_vol\" in market_state:\n",
    "                        strategy = random.choice([0, 1])  # Conservative or contrarian in high vol\n",
    "                    elif \"up\" in market_state:\n",
    "                        strategy = random.choice([2, 4])  # Momentum or aggressive in uptrend\n",
    "                    else:\n",
    "                        strategy = random.choice([1, 3])  # Contrarian or adaptive otherwise\n",
    "        else:\n",
    "            # Exploitation: choose best Q-value, but prefer personality-matched strategies\n",
    "            q_values = agent['q_table'][market_state]\n",
    "            if q_values:\n",
    "                # Boost Q-values for preferred strategies\n",
    "                adjusted_q_values = dict(q_values)\n",
    "                for preferred_strategy in agent['preferred_strategies']:\n",
    "                    if preferred_strategy in adjusted_q_values:\n",
    "                        adjusted_q_values[preferred_strategy] += 0.1  # Personality bias\n",
    "                \n",
    "                strategy = max(adjusted_q_values.items(), key=lambda x: x[1])[0]\n",
    "            else:\n",
    "                # Default to preferred strategy if no Q-values exist\n",
    "                strategy = random.choice(agent['preferred_strategies'])\n",
    "        \n",
    "        return strategy\n",
    "    \n",
    "    def tournament_round(self, strategies, backtester, current_idx):\n",
    "        \"\"\"Run tournament between agents with personality-aware evaluation\"\"\"\n",
    "        market_state = self.get_market_state(strategies, current_idx)\n",
    "        \n",
    "        # Each agent selects a strategy\n",
    "        agent_strategies = {}\n",
    "        for agent_name in self.agents:\n",
    "            strategy = self.agent_select_strategy(agent_name, market_state)\n",
    "            agent_strategies[agent_name] = strategy\n",
    "        \n",
    "        # Evaluate strategies over short window\n",
    "        window_size = 10\n",
    "        start_idx = max(250, current_idx - window_size)\n",
    "        end_idx = min(current_idx + window_size, len(strategies.dates))\n",
    "        \n",
    "        agent_rewards = {}\n",
    "        \n",
    "        for agent_name, strategy_idx in agent_strategies.items():\n",
    "            try:\n",
    "                strategy_name = f'Strategy{strategy_idx + 1}'\n",
    "                \n",
    "                weights_list = []\n",
    "                dates_list = []\n",
    "                \n",
    "                for i in range(start_idx, end_idx):\n",
    "                    if i < len(strategies.dates):\n",
    "                        strategy_func = getattr(strategies, f'task1_{strategy_name}')\n",
    "                        weights = strategy_func(i)\n",
    "                        weights_list.append(weights.values)\n",
    "                        dates_list.append(strategies.dates[i])\n",
    "                \n",
    "                if weights_list:\n",
    "                    weights_df = pd.DataFrame(weights_list, index=dates_list, columns=strategies.symbols)\n",
    "                    returns = backtester.calculate_returns(weights_df)\n",
    "                    performance = backtester.calculate_performance_metrics(returns)\n",
    "                    \n",
    "                    # Personality-adjusted reward calculation\n",
    "                    base_reward = (\n",
    "                        performance.get('sharpe_ratio', 0) * 0.5 +\n",
    "                        performance.get('total_return', 0) * 0.3 +\n",
    "                        (1 - abs(performance.get('max_drawdown', 0))) * 0.2\n",
    "                    )\n",
    "                    \n",
    "                    # Bonus for using preferred strategy\n",
    "                    personality_bonus = 0.05 if strategy_idx in self.agents[agent_name]['preferred_strategies'] else 0\n",
    "                    \n",
    "                    agent_rewards[agent_name] = base_reward + personality_bonus\n",
    "                else:\n",
    "                    agent_rewards[agent_name] = -0.1\n",
    "            except:\n",
    "                agent_rewards[agent_name] = -0.1\n",
    "        \n",
    "        # Update Q-tables and agent stats\n",
    "        for agent_name, reward in agent_rewards.items():\n",
    "            agent = self.agents[agent_name]\n",
    "            strategy = agent_strategies[agent_name]\n",
    "            \n",
    "            # Q-learning update with personality-aware learning rate\n",
    "            personality_lr_modifier = 1.2 if strategy in agent['preferred_strategies'] else 0.8\n",
    "            effective_lr = self.lr * personality_lr_modifier\n",
    "            \n",
    "            old_q = agent['q_table'][market_state][strategy]\n",
    "            agent['q_table'][market_state][strategy] = old_q + effective_lr * (reward - old_q)\n",
    "            \n",
    "            # Update agent stats\n",
    "            agent['recent_rewards'].append(reward)\n",
    "            agent['total_games'] += 1\n",
    "            \n",
    "            # Update strategy preferences with personality bias\n",
    "            decay = 0.95\n",
    "            agent['strategy_preferences'] *= decay\n",
    "            \n",
    "            # Stronger preference update for personality-matched strategies\n",
    "            preference_boost = 1.5 if strategy in agent['preferred_strategies'] else 1.0\n",
    "            agent['strategy_preferences'][strategy] += (1 - decay) * max(0, reward) * preference_boost\n",
    "            agent['strategy_preferences'] /= agent['strategy_preferences'].sum()\n",
    "        \n",
    "        # Determine winner\n",
    "        winner = max(agent_rewards.items(), key=lambda x: x[1])\n",
    "        self.agents[winner[0]]['wins'] += 1\n",
    "        \n",
    "        return winner[0], agent_strategies[winner[0]], winner[1]\n",
    "    \n",
    "    def train_tournament(self, strategies, backtester, train_start=250, train_end=None):\n",
    "        \"\"\"Train agents through tournaments with personality tracking\"\"\"\n",
    "        print(\"Training Multi-Agent RL Tournament with Personality-Strategy Mapping...\")\n",
    "        \n",
    "        if train_end is None:\n",
    "            train_end = len(strategies.dates) - 50\n",
    "        \n",
    "        tournament_results = []\n",
    "        personality_performance = defaultdict(list)\n",
    "        \n",
    "        for i in range(train_start, train_end, 20):\n",
    "            winner_agent, winning_strategy, reward = self.tournament_round(strategies, backtester, i)\n",
    "            \n",
    "            # Track personality performance\n",
    "            winner_personality = self.agents[winner_agent]['personality']\n",
    "            personality_performance[winner_personality].append(reward)\n",
    "            \n",
    "            tournament_results.append({\n",
    "                'index': i,\n",
    "                'winner_agent': winner_agent,\n",
    "                'winner_personality': winner_personality,\n",
    "                'winning_strategy': winning_strategy,\n",
    "                'reward': reward,\n",
    "                'strategy_personality_match': winning_strategy in self.personality_strategy_map[winner_personality]\n",
    "            })\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                match_indicator = \"â\" if winning_strategy in self.personality_strategy_map[winner_personality] else \"â\"\n",
    "                print(f\"  Tournament at index {i}: {winner_agent} ({winner_personality}) wins with Strategy{winning_strategy+1} {match_indicator} (reward: {reward:.4f})\")\n",
    "        \n",
    "        # Print final agent and personality statistics\n",
    "        print(\"\\nFinal Agent Statistics:\")\n",
    "        for agent_name, agent in self.agents.items():\n",
    "            win_rate = agent['wins'] / agent['total_games'] if agent['total_games'] > 0 else 0\n",
    "            avg_reward = np.mean(agent['recent_rewards']) if agent['recent_rewards'] else 0\n",
    "            preferred_strats = [f\"S{s+1}\" for s in agent['preferred_strategies']]\n",
    "            print(f\"  {agent_name} ({agent['personality']}): {win_rate:.2%} win rate, {avg_reward:.4f} avg reward, prefers {preferred_strats}\")\n",
    "        \n",
    "        print(\"\\nPersonality Performance Summary:\")\n",
    "        for personality, rewards in personality_performance.items():\n",
    "            avg_reward = np.mean(rewards)\n",
    "            win_count = len(rewards)\n",
    "            preferred_strategy = self.personality_strategy_map[personality][0] + 1\n",
    "            print(f\"  {personality}: {win_count} wins, {avg_reward:.4f} avg reward, prefers Strategy{preferred_strategy}\")\n",
    "        \n",
    "        # Calculate strategy-personality alignment rate\n",
    "        matches = sum(1 for result in tournament_results if result['strategy_personality_match'])\n",
    "        alignment_rate = matches / len(tournament_results) if tournament_results else 0\n",
    "        print(f\"\\nStrategy-Personality Alignment Rate: {alignment_rate:.2%}\")\n",
    "        \n",
    "        self.tournament_results = tournament_results\n",
    "        self.personality_performance = dict(personality_performance)\n",
    "    \n",
    "    def predict_strategy(self, strategies, current_idx):\n",
    "        \"\"\"Predict using ensemble of agents with personality weighting\"\"\"\n",
    "        market_state = self.get_market_state(strategies, current_idx)\n",
    "        \n",
    "        # Get strategy votes from all agents\n",
    "        strategy_votes = defaultdict(float)\n",
    "        personality_confidence = defaultdict(float)\n",
    "        \n",
    "        for agent_name, agent in self.agents.items():\n",
    "            strategy = self.agent_select_strategy(agent_name, market_state)\n",
    "            \n",
    "            # Weight vote by agent's recent performance\n",
    "            base_weight = np.mean(agent['recent_rewards']) if agent['recent_rewards'] else 0\n",
    "            base_weight = max(0.1, base_weight)  # Minimum weight\n",
    "            \n",
    "            # Boost weight if using preferred strategy\n",
    "            personality_boost = 1.3 if strategy in agent['preferred_strategies'] else 1.0\n",
    "            final_weight = base_weight * personality_boost\n",
    "            \n",
    "            strategy_votes[strategy] += final_weight\n",
    "            personality_confidence[agent['personality']] += final_weight\n",
    "        \n",
    "        # Select strategy with highest weighted votes\n",
    "        if strategy_votes:\n",
    "            best_strategy = max(strategy_votes.items(), key=lambda x: x[1])[0]\n",
    "            confidence = strategy_votes[best_strategy] / sum(strategy_votes.values())\n",
    "            \n",
    "            # Determine which personality type is driving the decision\n",
    "            dominant_personality = max(personality_confidence.items(), key=lambda x: x[1])[0]\n",
    "            \n",
    "            return best_strategy, confidence, dominant_personality\n",
    "        else:\n",
    "            return 0, 0.2, 'conservative'  # Safe default\n",
    "    \n",
    "    def get_personality_insights(self):\n",
    "        \"\"\"Get insights about personality performance and strategy preferences\"\"\"\n",
    "        insights = {}\n",
    "        \n",
    "        for agent_name, agent in self.agents.items():\n",
    "            personality = agent['personality']\n",
    "            preferred_strategies = agent['preferred_strategies']\n",
    "            \n",
    "            # Calculate strategy usage distribution\n",
    "            strategy_usage = defaultdict(int)\n",
    "            for state, strategies in agent['q_table'].items():\n",
    "                for strategy, q_value in strategies.items():\n",
    "                    strategy_usage[strategy] += 1\n",
    "            \n",
    "            insights[agent_name] = {\n",
    "                'personality': personality,\n",
    "                'preferred_strategies': [f\"Strategy{s+1}\" for s in preferred_strategies],\n",
    "                'win_rate': agent['wins'] / agent['total_games'] if agent['total_games'] > 0 else 0,\n",
    "                'avg_recent_reward': np.mean(agent['recent_rewards']) if agent['recent_rewards'] else 0,\n",
    "                'strategy_usage': dict(strategy_usage),\n",
    "                'total_games': agent['total_games']\n",
    "            }\n",
    "        \n",
    "        return insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task2(train_data_path='train_data.csv', test_data_path='test_data.csv'):\n",
    "    \"\"\"\n",
    "    Task 2: Multi-Agent RL Tournament Strategy Selection\n",
    "    \"\"\"\n",
    "    print(\"=== TASK 2: Multi-Agent RL Tournament Strategy Selection ===\")\n",
    "    \n",
    "    # Load data\n",
    "    train_data = pd.read_csv(train_data_path)\n",
    "    train_data = train_data.iloc[:, 1:]\n",
    "    \n",
    "    test_data = pd.read_csv(test_data_path)\n",
    "    test_data = test_data.iloc[:, 1:]\n",
    "    \n",
    "    print(f\"Training data shape: {train_data.shape}\")\n",
    "    print(f\"Test data shape: {test_data.shape}\")\n",
    "    \n",
    "    # TRAINING PHASE\n",
    "    print(\"\\n--- TRAINING PHASE ---\")\n",
    "    train_strategies = TradingStrategies(train_data)\n",
    "    train_strategies.prepare_data()\n",
    "    \n",
    "    train_backtester = StrategyBacktester(train_data, train_strategies)\n",
    "    \n",
    "    # Initialize Multi-Agent RL selector\n",
    "    marl_selector = MultiAgentRLSelector(n_agents=5, learning_rate=0.1, epsilon=0.2)\n",
    "    \n",
    "    # Train through tournaments\n",
    "    marl_selector.train_tournament(\n",
    "        train_strategies, \n",
    "        train_backtester,\n",
    "        train_start=250,\n",
    "        train_end=len(train_strategies.dates) - 50\n",
    "    )\n",
    "    \n",
    "    # TESTING PHASE\n",
    "    print(\"\\n--- TESTING PHASE ---\")\n",
    "    test_strategies = TradingStrategies(test_data)\n",
    "    test_strategies.prepare_data()\n",
    "    \n",
    "    test_backtester = StrategyBacktester(test_data, test_strategies)\n",
    "    \n",
    "    # Apply trained agents to test data\n",
    "    ensemble_weights = []\n",
    "    ensemble_dates = []\n",
    "    strategy_selections = []\n",
    "    prediction_confidences = []\n",
    "    \n",
    "    print(\"Generating predictions with trained agents...\")\n",
    "    \n",
    "    LOOKBACK_REQUIRED = 250\n",
    "    \n",
    "    if len(test_strategies.dates) < LOOKBACK_REQUIRED:\n",
    "        raise ValueError(f\"Test data insufficient. Need at least {LOOKBACK_REQUIRED} days\")\n",
    "    \n",
    "    for i in range(LOOKBACK_REQUIRED, len(test_strategies.dates)):\n",
    "        date = test_strategies.dates[i]\n",
    "        \n",
    "        # Get prediction from agent ensemble - Fixed: now properly unpacking 3 values\n",
    "        strategy_idx, confidence, dominant_personality = marl_selector.predict_strategy(test_strategies, i)\n",
    "        strategy_name = f'Strategy{strategy_idx + 1}'\n",
    "        \n",
    "        # Get weights\n",
    "        strategy_func = getattr(test_strategies, f'task1_{strategy_name}')\n",
    "        weights = strategy_func(i)\n",
    "        \n",
    "        ensemble_weights.append(weights.values)\n",
    "        ensemble_dates.append(date)\n",
    "        strategy_selections.append(strategy_name)\n",
    "        prediction_confidences.append(confidence)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f\"  Day {i-LOOKBACK_REQUIRED+1}: {strategy_name} (confidence: {confidence:.3f})\")\n",
    "    \n",
    "    # Create results\n",
    "    ensemble_weights_df = pd.DataFrame(\n",
    "        ensemble_weights,\n",
    "        index=ensemble_dates,\n",
    "        columns=test_strategies.symbols\n",
    "    )\n",
    "    \n",
    "    # Calculate performance\n",
    "    ensemble_returns = test_backtester.calculate_returns(ensemble_weights_df)\n",
    "    ensemble_performance = test_backtester.calculate_performance_metrics(ensemble_returns)\n",
    "    \n",
    "    print(\"\\nMulti-Agent RL Ensemble Performance:\")\n",
    "    for metric, value in ensemble_performance.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    \n",
    "    # Save results\n",
    "    ensemble_performance_df = pd.DataFrame([ensemble_performance])\n",
    "    ensemble_performance_df.to_csv('backtest_performance_metrics_rl.csv', index=False)\n",
    "    \n",
    "    ensemble_weights_df.to_csv('ensemble_weights_rl.csv')\n",
    "    \n",
    "    performance_df = pd.DataFrame({\n",
    "        'Date': ensemble_dates,\n",
    "        'Returns': ensemble_returns.values,\n",
    "        'Cumulative_Returns': (1 + ensemble_returns).cumprod().values,\n",
    "        'Selected_Strategy': strategy_selections,\n",
    "        'Prediction_Confidence': prediction_confidences\n",
    "    })\n",
    "    performance_df.to_csv('ensemble_performance_rl.csv', index=False)\n",
    "    \n",
    "    # Save model\n",
    "    model_data = {\n",
    "        'marl_selector': marl_selector,\n",
    "        'training_results': marl_selector.tournament_results if hasattr(marl_selector, 'tournament_results') else []\n",
    "    }\n",
    "    \n",
    "    # with open('ensemble_model_rl.pkl', 'wb') as f:\n",
    "    #     pickle.dump(model_data, f)\n",
    "    \n",
    "    return ensemble_weights_df, ensemble_performance, model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TASK 2: Multi-Agent RL Tournament Strategy Selection ===\n",
      "Training data shape: (70000, 14)\n",
      "Test data shape: (11120, 14)\n",
      "\n",
      "--- TRAINING PHASE ---\n",
      "Training Multi-Agent RL Tournament with Personality-Strategy Mapping...\n",
      "\n",
      "Final Agent Statistics:\n",
      "  Agent_0 (conservative): 13.12% win rate, 1.4105 avg reward, prefers ['S1']\n",
      "  Agent_1 (aggressive): 26.25% win rate, 3.6005 avg reward, prefers ['S5']\n",
      "  Agent_2 (momentum): 20.62% win rate, 2.2784 avg reward, prefers ['S3']\n",
      "  Agent_3 (contrarian): 25.62% win rate, 2.8366 avg reward, prefers ['S2']\n",
      "  Agent_4 (adaptive): 14.37% win rate, 1.5003 avg reward, prefers ['S4']\n",
      "\n",
      "Personality Performance Summary:\n",
      "  adaptive: 23 wins, 5.8157 avg reward, prefers Strategy4\n",
      "  contrarian: 41 wins, 6.3756 avg reward, prefers Strategy2\n",
      "  momentum: 33 wins, 5.6666 avg reward, prefers Strategy3\n",
      "  aggressive: 42 wins, 15.0173 avg reward, prefers Strategy5\n",
      "  conservative: 21 wins, 4.5703 avg reward, prefers Strategy1\n",
      "\n",
      "Strategy-Personality Alignment Rate: 98.75%\n",
      "\n",
      "--- TESTING PHASE ---\n",
      "Generating predictions with trained agents...\n",
      "  Day 51: Strategy5 (confidence: 0.310)\n",
      "  Day 151: Strategy5 (confidence: 0.310)\n",
      "  Day 251: Strategy5 (confidence: 0.310)\n",
      "\n",
      "Multi-Agent RL Ensemble Performance:\n",
      "  total_return: 0.0862\n",
      "  annualized_return: 0.1192\n",
      "  volatility: 0.2988\n",
      "  sharpe_ratio: 0.3988\n",
      "  max_drawdown: 0.2108\n",
      "\n",
      "RL epsilon greedy-based ensemble strategy completed successfully!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    ensemble_weights, ensemble_performance, model_data = task2(\n",
    "        train_data_path='train_data.csv',\n",
    "        test_data_path='test_data.csv'\n",
    "    )\n",
    "    \n",
    "    print(\"\\nRL epsilon greedy-based ensemble strategy completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque, defaultdict\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "class DQNNetwork(nn.Module):\n",
    "    \"\"\"Deep Q-Network for strategy selection\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, hidden_size=128):\n",
    "        super(DQNNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, action_size)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "class DQNAgent:\n",
    "    \"\"\"DQN Agent with experience replay and target network\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, lr=0.001, gamma=0.95, epsilon=1.0, \n",
    "                 epsilon_decay=0.995, epsilon_min=0.01, memory_size=10000, batch_size=32):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.batch_size = batch_size\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Neural networks\n",
    "        self.q_network = DQNNetwork(state_size, action_size).to(self.device)\n",
    "        self.target_network = DQNNetwork(state_size, action_size).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        \n",
    "        # Initialize target network\n",
    "        self.update_target_network()\n",
    "        \n",
    "    def update_target_network(self):\n",
    "        \"\"\"Copy weights from main network to target network\"\"\"\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience in replay memory\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def act(self, state, personality_bias=None):\n",
    "        \"\"\"Choose action using epsilon-greedy policy with personality bias\"\"\"\n",
    "        if np.random.random() <= self.epsilon:\n",
    "            if personality_bias is not None:\n",
    "                # Weighted random selection based on personality\n",
    "                return np.random.choice(self.action_size, p=personality_bias)\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        q_values = self.q_network(state_tensor)\n",
    "        \n",
    "        # Apply personality bias to Q-values\n",
    "        if personality_bias is not None:\n",
    "            bias_tensor = torch.FloatTensor(personality_bias).to(self.device)\n",
    "            q_values = q_values + bias_tensor * 0.1\n",
    "        \n",
    "        return np.argmax(q_values.cpu().data.numpy())\n",
    "    \n",
    "    def replay(self):\n",
    "        \"\"\"Train the model on a batch of experiences\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states = torch.FloatTensor([e[0] for e in batch]).to(self.device)\n",
    "        actions = torch.LongTensor([e[1] for e in batch]).to(self.device)\n",
    "        rewards = torch.FloatTensor([e[2] for e in batch]).to(self.device)\n",
    "        next_states = torch.FloatTensor([e[3] for e in batch]).to(self.device)\n",
    "        dones = torch.BoolTensor([e[4] for e in batch]).to(self.device)\n",
    "        \n",
    "        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))\n",
    "        next_q_values = self.target_network(next_states).max(1)[0].detach()\n",
    "        target_q_values = rewards + (self.gamma * next_q_values * ~dones)\n",
    "        \n",
    "        loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "class AdvancedDQNSelector:\n",
    "    \n",
    "    def __init__(self, n_agents=5, state_size=15, action_size=5):\n",
    "        self.n_agents = n_agents\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # Personality mappings\n",
    "        self.personality_strategy_map = {\n",
    "            'conservative': [0],\n",
    "            'contrarian': [1],\n",
    "            'momentum': [2],\n",
    "            'adaptive': [3],\n",
    "            'aggressive': [4]\n",
    "        }\n",
    "        \n",
    "        # Initialize agents\n",
    "        self.agents = {}\n",
    "        personalities = ['conservative', 'aggressive', 'momentum', 'contrarian', 'adaptive']\n",
    "        \n",
    "        for i in range(n_agents):\n",
    "            personality = personalities[i % len(personalities)]\n",
    "            \n",
    "            # Create personality bias (probability distribution)\n",
    "            bias = np.ones(action_size) * 0.1\n",
    "            preferred_strategies = self.personality_strategy_map[personality]\n",
    "            for strategy in preferred_strategies:\n",
    "                bias[strategy] = 0.4\n",
    "            bias = bias / bias.sum()\n",
    "            \n",
    "            self.agents[f'Agent_{i}'] = {\n",
    "                'dqn': DQNAgent(state_size, action_size),\n",
    "                'personality': personality,\n",
    "                'personality_bias': bias,\n",
    "                'wins': 0,\n",
    "                'total_games': 0,\n",
    "                'recent_rewards': deque(maxlen=20)\n",
    "            }\n",
    "    \n",
    "    def get_market_state_vector(self, strategies, current_idx):\n",
    "        \"\"\"Convert market conditions to feature vector\"\"\"\n",
    "        if current_idx < 50:\n",
    "            return np.zeros(self.state_size)\n",
    "        \n",
    "        features = []\n",
    "        \n",
    "        # Price-based features\n",
    "        recent_prices = strategies.close_prices.iloc[current_idx-20:current_idx]\n",
    "        returns = recent_prices.pct_change().fillna(0)\n",
    "        \n",
    "        # Returns statistics\n",
    "        features.extend([\n",
    "            returns.mean().mean(),\n",
    "            returns.std().mean(),\n",
    "            returns.skew().mean(),\n",
    "            returns.kurt().mean()\n",
    "        ])\n",
    "        \n",
    "        # Trend features\n",
    "        short_ma = recent_prices.rolling(5).mean().iloc[-1]\n",
    "        long_ma = recent_prices.rolling(20).mean().iloc[-1]\n",
    "        trend = ((short_ma / long_ma) - 1).mean()\n",
    "        features.append(trend)\n",
    "        \n",
    "        # Volatility features\n",
    "        volatility = returns.rolling(5).std().iloc[-1].mean()\n",
    "        features.append(volatility)\n",
    "        \n",
    "        # Momentum features\n",
    "        momentum_5 = (recent_prices.iloc[-1] / recent_prices.iloc[-5] - 1).mean()\n",
    "        momentum_10 = (recent_prices.iloc[-1] / recent_prices.iloc[-10] - 1).mean()\n",
    "        features.extend([momentum_5, momentum_10])\n",
    "        \n",
    "        # Volume-based features (if available)\n",
    "        if hasattr(strategies, 'volume_data'):\n",
    "            volume_ma = strategies.volume_data.iloc[current_idx-5:current_idx].mean().mean()\n",
    "            volume_std = strategies.volume_data.iloc[current_idx-5:current_idx].std().mean()\n",
    "            features.extend([volume_ma, volume_std])\n",
    "        else:\n",
    "            features.extend([0, 0])\n",
    "        \n",
    "        # RSI-like feature\n",
    "        gains = returns[returns > 0].mean().mean()\n",
    "        losses = abs(returns[returns < 0]).mean().mean()\n",
    "        rsi = gains / (gains + losses) if (gains + losses) > 0 else 0.5\n",
    "        features.append(rsi)\n",
    "        \n",
    "        # Market regime features\n",
    "        high_vol = 1 if volatility > 0.02 else 0\n",
    "        uptrend = 1 if trend > 0.01 else 0\n",
    "        downtrend = 1 if trend < -0.01 else 0\n",
    "        features.extend([high_vol, uptrend, downtrend])\n",
    "        \n",
    "        # Pad or truncate to exact size\n",
    "        features = features[:self.state_size]\n",
    "        while len(features) < self.state_size:\n",
    "            features.append(0)\n",
    "        \n",
    "        return np.array(features, dtype=np.float32)\n",
    "    \n",
    "    def train_tournament(self, strategies, backtester, train_start=250, train_end=None):\n",
    "        \"\"\"Train DQN agents through tournaments\"\"\"\n",
    "        print(\"Training Advanced DQN Multi-Agent Tournament...\")\n",
    "        \n",
    "        if train_end is None:\n",
    "            train_end = len(strategies.dates) - 50\n",
    "        \n",
    "        tournament_results = []\n",
    "        update_target_frequency = 100\n",
    "        step_count = 0\n",
    "        \n",
    "        for i in range(train_start, train_end, 10):\n",
    "            state = self.get_market_state_vector(strategies, i)\n",
    "            \n",
    "            # Each agent selects action\n",
    "            agent_actions = {}\n",
    "            for agent_name, agent_data in self.agents.items():\n",
    "                action = agent_data['dqn'].act(state, agent_data['personality_bias'])\n",
    "                agent_actions[agent_name] = action\n",
    "            \n",
    "            # Evaluate strategies\n",
    "            agent_rewards = self.evaluate_strategies(strategies, backtester, agent_actions, i)\n",
    "            \n",
    "            # Get next state\n",
    "            next_i = min(i + 10, train_end - 1)\n",
    "            next_state = self.get_market_state_vector(strategies, next_i)\n",
    "            done = (next_i >= train_end - 1)\n",
    "            \n",
    "            # Store experiences and train\n",
    "            for agent_name, agent_data in self.agents.items():\n",
    "                action = agent_actions[agent_name]\n",
    "                reward = agent_rewards[agent_name]\n",
    "                \n",
    "                agent_data['dqn'].remember(state, action, reward, next_state, done)\n",
    "                agent_data['dqn'].replay()\n",
    "                \n",
    "                # Update stats\n",
    "                agent_data['recent_rewards'].append(reward)\n",
    "                agent_data['total_games'] += 1\n",
    "            \n",
    "            # Update target networks periodically\n",
    "            step_count += 1\n",
    "            if step_count % update_target_frequency == 0:\n",
    "                for agent_data in self.agents.values():\n",
    "                    agent_data['dqn'].update_target_network()\n",
    "            \n",
    "            # Track winner\n",
    "            winner = max(agent_rewards.items(), key=lambda x: x[1])\n",
    "            self.agents[winner[0]]['wins'] += 1\n",
    "            \n",
    "            tournament_results.append({\n",
    "                'index': i,\n",
    "                'winner_agent': winner[0],\n",
    "                'winning_strategy': agent_actions[winner[0]],\n",
    "                'reward': winner[1]\n",
    "            })\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                avg_reward = np.mean(list(agent_rewards.values()))\n",
    "                print(f\"  Step {i}: Winner {winner[0]}, Strategy {winner[1]}, Avg Reward: {avg_reward:.4f}\")\n",
    "        \n",
    "        self.tournament_results = tournament_results\n",
    "        print(\"DQN Training completed!\")\n",
    "    \n",
    "    def evaluate_strategies(self, strategies, backtester, agent_actions, current_idx):\n",
    "        \"\"\"Evaluate agent strategies and return rewards\"\"\"\n",
    "        window_size = 10\n",
    "        start_idx = max(250, current_idx - window_size)\n",
    "        end_idx = min(current_idx + window_size, len(strategies.dates))\n",
    "        \n",
    "        agent_rewards = {}\n",
    "        \n",
    "        for agent_name, strategy_idx in agent_actions.items():\n",
    "            try:\n",
    "                strategy_name = f'Strategy{strategy_idx + 1}'\n",
    "                \n",
    "                weights_list = []\n",
    "                dates_list = []\n",
    "                \n",
    "                for i in range(start_idx, end_idx):\n",
    "                    if i < len(strategies.dates):\n",
    "                        strategy_func = getattr(strategies, f'task1_{strategy_name}')\n",
    "                        weights = strategy_func(i)\n",
    "                        weights_list.append(weights.values)\n",
    "                        dates_list.append(strategies.dates[i])\n",
    "                \n",
    "                if weights_list:\n",
    "                    weights_df = pd.DataFrame(weights_list, index=dates_list, columns=strategies.symbols)\n",
    "                    returns = backtester.calculate_returns(weights_df)\n",
    "                    performance = backtester.calculate_performance_metrics(returns)\n",
    "                    \n",
    "                    reward = (\n",
    "                        performance.get('sharpe_ratio', 0) * 0.5 +\n",
    "                        performance.get('total_return', 0) * 0.3 +\n",
    "                        (1 - abs(performance.get('max_drawdown', 0))) * 0.2\n",
    "                    )\n",
    "                    agent_rewards[agent_name] = reward\n",
    "                else:\n",
    "                    agent_rewards[agent_name] = -0.1\n",
    "            except:\n",
    "                agent_rewards[agent_name] = -0.1\n",
    "        \n",
    "        return agent_rewards\n",
    "    \n",
    "    def predict_strategy(self, strategies, current_idx):\n",
    "        \"\"\"Predict using ensemble of DQN agents\"\"\"\n",
    "        state = self.get_market_state_vector(strategies, current_idx)\n",
    "        \n",
    "        strategy_votes = defaultdict(float)\n",
    "        \n",
    "        for agent_name, agent_data in self.agents.items():\n",
    "            # Set epsilon to 0 for prediction (no exploration)\n",
    "            original_epsilon = agent_data['dqn'].epsilon\n",
    "            agent_data['dqn'].epsilon = 0\n",
    "            \n",
    "            action = agent_data['dqn'].act(state, agent_data['personality_bias'])\n",
    "            \n",
    "            # Weight by recent performance\n",
    "            weight = np.mean(agent_data['recent_rewards']) if agent_data['recent_rewards'] else 0\n",
    "            weight = max(0.1, weight)\n",
    "            \n",
    "            strategy_votes[action] += weight\n",
    "            \n",
    "            # Restore original epsilon\n",
    "            agent_data['dqn'].epsilon = original_epsilon\n",
    "        \n",
    "        if strategy_votes:\n",
    "            best_strategy = max(strategy_votes.items(), key=lambda x: x[1])[0]\n",
    "            confidence = strategy_votes[best_strategy] / sum(strategy_votes.values())\n",
    "            return best_strategy, confidence\n",
    "        else:\n",
    "            return 0, 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task2_advanced(train_data_path='train_data.csv', test_data_path='test_data.csv', method='DQN'):\n",
    "    \"\"\"\n",
    "    Task 2: Advanced RL Tournament Strategy Selection\n",
    "    Methods: 'DQN', 'PPO'\n",
    "    \"\"\"\n",
    "    print(f\"=== TASK 2: Advanced {method} Multi-Agent Strategy Selection ===\")\n",
    "    \n",
    "    # Load data\n",
    "    train_data = pd.read_csv(train_data_path)\n",
    "    train_data = train_data.iloc[:, 1:]\n",
    "    \n",
    "    test_data = pd.read_csv(test_data_path)\n",
    "    test_data = test_data.iloc[:, 1:]\n",
    "    \n",
    "    print(f\"Training data shape: {train_data.shape}\")\n",
    "    print(f\"Test data shape: {test_data.shape}\")\n",
    "    \n",
    "    # TRAINING PHASE\n",
    "    print(\"\\n--- TRAINING PHASE ---\")\n",
    "    train_strategies = TradingStrategies(train_data)\n",
    "    train_strategies.prepare_data()\n",
    "    \n",
    "    train_backtester = StrategyBacktester(train_data, train_strategies)\n",
    "    \n",
    "    # Initialize Advanced RL selector based on method\n",
    "    if method == 'DQN':\n",
    "        rl_selector = AdvancedDQNSelector(n_agents=5, state_size=15, action_size=5)\n",
    "    elif method == 'PPO':\n",
    "        rl_selector = AdvancedPPOSelector(n_agents=5, state_size=15, action_size=5)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}. Choose 'DQN' or 'PPO'\")\n",
    "    \n",
    "    # Train through tournaments\n",
    "    rl_selector.train_tournament(\n",
    "        train_strategies, \n",
    "        train_backtester,\n",
    "        train_start=250,\n",
    "        train_end=len(train_strategies.dates) - 50\n",
    "    )\n",
    "    \n",
    "    # TESTING PHASE\n",
    "    print(\"\\n--- TESTING PHASE ---\")\n",
    "    test_strategies = TradingStrategies(test_data)\n",
    "    test_strategies.prepare_data()\n",
    "    \n",
    "    test_backtester = StrategyBacktester(test_data, test_strategies)\n",
    "    \n",
    "    # Apply trained agents to test data\n",
    "    ensemble_weights = []\n",
    "    ensemble_dates = []\n",
    "    strategy_selections = []\n",
    "    prediction_confidences = []\n",
    "    \n",
    "    print(\"Generating predictions with trained agents...\")\n",
    "    \n",
    "    LOOKBACK_REQUIRED = 250\n",
    "    \n",
    "    if len(test_strategies.dates) < LOOKBACK_REQUIRED:\n",
    "        raise ValueError(f\"Test data insufficient. Need at least {LOOKBACK_REQUIRED} days\")\n",
    "    \n",
    "    for i in range(LOOKBACK_REQUIRED, len(test_strategies.dates)):\n",
    "        date = test_strategies.dates[i]\n",
    "        \n",
    "        strategy_idx, confidence = rl_selector.predict_strategy(test_strategies, i)\n",
    "        strategy_name = f'Strategy{strategy_idx + 1}'\n",
    "        \n",
    "        # Get weights\n",
    "        strategy_func = getattr(test_strategies, f'task1_{strategy_name}')\n",
    "        weights = strategy_func(i)\n",
    "        \n",
    "        ensemble_weights.append(weights.values)\n",
    "        ensemble_dates.append(date)\n",
    "        strategy_selections.append(strategy_name)\n",
    "        prediction_confidences.append(confidence)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f\"  Day {i-LOOKBACK_REQUIRED+1}: {strategy_name} (confidence: {confidence:.3f})\")\n",
    "    \n",
    "    # Create results\n",
    "    ensemble_weights_df = pd.DataFrame(\n",
    "        ensemble_weights,\n",
    "        index=ensemble_dates,\n",
    "        columns=test_strategies.symbols\n",
    "    )\n",
    "    \n",
    "    # Calculate performance\n",
    "    ensemble_returns = test_backtester.calculate_returns(ensemble_weights_df)\n",
    "    ensemble_performance = test_backtester.calculate_performance_metrics(ensemble_returns)\n",
    "    \n",
    "    print(f\"\\n{method} Ensemble Performance:\")\n",
    "    for metric, value in ensemble_performance.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    \n",
    "    # Save results\n",
    "    ensemble_performance_df = pd.DataFrame([ensemble_performance])\n",
    "    ensemble_performance_df.to_csv(f'backtest_performance_metrics_{method.lower()}.csv', index=False)\n",
    "    \n",
    "    ensemble_weights_df.to_csv(f'ensemble_weights_{method.lower()}.csv')\n",
    "    \n",
    "    performance_df = pd.DataFrame({\n",
    "        'Date': ensemble_dates,\n",
    "        'Returns': ensemble_returns.values,\n",
    "        'Cumulative_Returns': (1 + ensemble_returns).cumprod().values,\n",
    "        'Selected_Strategy': strategy_selections,\n",
    "        'Prediction_Confidence': prediction_confidences\n",
    "    })\n",
    "    performance_df.to_csv(f'ensemble_performance_{method.lower()}.csv', index=False)\n",
    "    \n",
    "    # Save model\n",
    "    model_data = {\n",
    "        'rl_selector': rl_selector,\n",
    "        'method': method,\n",
    "        'training_results': rl_selector.tournament_results if hasattr(rl_selector, 'tournament_results') else []\n",
    "    }\n",
    "    \n",
    "    with open(f'ensemble_model_{method.lower()}.pkl', 'wb') as f:\n",
    "        pickle.dump(model_data, f)\n",
    "    \n",
    "    return ensemble_weights_df, ensemble_performance, model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DQN method...\n",
      "=== TASK 2: Advanced DQN Multi-Agent Strategy Selection ===\n",
      "Training data shape: (70000, 14)\n",
      "Test data shape: (10000, 14)\n",
      "\n",
      "--- TRAINING PHASE ---\n",
      "Training Advanced DQN Multi-Agent Tournament...\n",
      "  Step 300: Winner Agent_2, Strategy 10.69268617515505, Avg Reward: 4.1262\n",
      "  Step 400: Winner Agent_3, Strategy 19.09849967646336, Avg Reward: 6.9748\n",
      "  Step 500: Winner Agent_0, Strategy 4.144673029075346, Avg Reward: 1.7357\n",
      "  Step 600: Winner Agent_3, Strategy 7.980537078421094, Avg Reward: 3.1261\n",
      "  Step 700: Winner Agent_0, Strategy 3.8746701712487726, Avg Reward: 2.2265\n",
      "  Step 800: Winner Agent_1, Strategy -0.5330969115133646, Avg Reward: -0.7787\n",
      "  Step 900: Winner Agent_1, Strategy 6.857081049077467, Avg Reward: 3.3095\n",
      "  Step 1000: Winner Agent_4, Strategy 16.880271038853493, Avg Reward: 4.2498\n",
      "  Step 1100: Winner Agent_0, Strategy -1.6601180303126826, Avg Reward: -2.0561\n",
      "  Step 1200: Winner Agent_0, Strategy 8.080165196510615, Avg Reward: 8.0802\n",
      "  Step 1300: Winner Agent_2, Strategy 3.1192527452780174, Avg Reward: 1.5627\n",
      "  Step 1400: Winner Agent_2, Strategy 2.8290989048492516, Avg Reward: 1.7810\n",
      "  Step 1500: Winner Agent_1, Strategy 10.130424380676258, Avg Reward: 7.7284\n",
      "  Step 1600: Winner Agent_3, Strategy 4.150809407405606, Avg Reward: 3.4282\n",
      "  Step 1700: Winner Agent_3, Strategy 2.6554084297619216, Avg Reward: 0.5533\n",
      "  Step 1800: Winner Agent_4, Strategy 2.322014381372084, Avg Reward: -0.2066\n",
      "  Step 1900: Winner Agent_2, Strategy 2.6184859502996547, Avg Reward: 1.0646\n",
      "  Step 2000: Winner Agent_4, Strategy 0.2051823532530692, Avg Reward: -0.3720\n",
      "  Step 2100: Winner Agent_1, Strategy -0.11735772447204307, Avg Reward: -0.6251\n",
      "  Step 2200: Winner Agent_1, Strategy 8.952595630031938, Avg Reward: 5.2693\n",
      "  Step 2300: Winner Agent_2, Strategy 4.015837060084379, Avg Reward: 0.7043\n",
      "  Step 2400: Winner Agent_1, Strategy 0.5921524297162548, Avg Reward: -0.8596\n",
      "  Step 2500: Winner Agent_0, Strategy 10.358306513837364, Avg Reward: 1.3087\n",
      "  Step 2600: Winner Agent_3, Strategy 1.221080950264746, Avg Reward: 0.1932\n",
      "  Step 2700: Winner Agent_1, Strategy 4.212373265871792, Avg Reward: 3.0936\n",
      "  Step 2800: Winner Agent_0, Strategy -0.660845858659981, Avg Reward: -0.9783\n",
      "  Step 2900: Winner Agent_2, Strategy 4.272964313026053, Avg Reward: 1.4489\n",
      "  Step 3000: Winner Agent_1, Strategy 10.370396578536603, Avg Reward: 4.6885\n",
      "  Step 3100: Winner Agent_1, Strategy 2.2444965683970963, Avg Reward: 1.2525\n",
      "  Step 3200: Winner Agent_1, Strategy 1.2952659179778994, Avg Reward: 0.4797\n",
      "  Step 3300: Winner Agent_2, Strategy 7.187209337783269, Avg Reward: 4.5013\n",
      "  Step 3400: Winner Agent_4, Strategy 2.315637969555041, Avg Reward: 1.7151\n",
      "DQN Training completed!\n",
      "\n",
      "--- TESTING PHASE ---\n",
      "Generating predictions with trained agents...\n",
      "  Day 51: Strategy3 (confidence: 0.314)\n",
      "  Day 151: Strategy5 (confidence: 0.520)\n",
      "\n",
      "DQN Ensemble Performance:\n",
      "  total_return: 0.1237\n",
      "  annualized_return: 0.1599\n",
      "  volatility: 0.2492\n",
      "  sharpe_ratio: 0.6418\n",
      "  max_drawdown: 0.1785\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "=== PERFORMANCE COMPARISON ===\n",
      "DQN Performance:\n",
      "  total_return: 0.1237\n",
      "  annualized_return: 0.1599\n",
      "  volatility: 0.2492\n",
      "  sharpe_ratio: 0.6418\n",
      "  max_drawdown: 0.1785\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Test DQN method\n",
    "    print(\"Testing DQN method...\")\n",
    "    ensemble_weights_dqn, ensemble_performance_dqn, model_data_dqn = task2_advanced(\n",
    "        train_data_path='train_data.csv',\n",
    "        test_data_path='cross_val_data.csv',\n",
    "        method='DQN'\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    \n",
    "    print(\"\\n=== PERFORMANCE COMPARISON ===\")\n",
    "    print(\"DQN Performance:\")\n",
    "    for metric, value in ensemble_performance_dqn.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DQN model with fixed epsilon...\n",
      "Fixing DQN model: ensemble_model_dqn.pkl\n",
      "  Fixed Agent_0: epsilon = 0.0\n",
      "  Fixed Agent_1: epsilon = 0.0\n",
      "  Fixed Agent_2: epsilon = 0.0\n",
      "  Fixed Agent_3: epsilon = 0.0\n",
      "  Fixed Agent_4: epsilon = 0.0\n",
      "Fixed model saved to: ensemble_model_dqn_fixed.pkl\n",
      "=== LOADING AND TESTING DQN MODEL ===\n",
      "Loading model from: ensemble_model_dqn_fixed.pkl\n",
      "Model loaded successfully! Method: DQN\n",
      "Number of agents: 5\n",
      "  Agent_0: epsilon set to 0.0\n",
      "  Agent_1: epsilon set to 0.0\n",
      "  Agent_2: epsilon set to 0.0\n",
      "  Agent_3: epsilon set to 0.0\n",
      "  Agent_4: epsilon set to 0.0\n",
      "  Agent_0: conservative personality, 15 games, 13.33% win rate\n",
      "  Agent_1: aggressive personality, 15 games, 40.00% win rate\n",
      "  Agent_2: momentum personality, 15 games, 6.67% win rate\n",
      "  Agent_3: contrarian personality, 15 games, 26.67% win rate\n",
      "  Agent_4: adaptive personality, 15 games, 13.33% win rate\n",
      "\n",
      "Loading test data from: cross_val_data.csv\n",
      "Test data shape: (10000, 14)\n",
      "\n",
      "--- TESTING PHASE ---\n",
      "Generating predictions with pre-trained agents...\n",
      "  Day 51: Strategy1 (confidence: 0.573)\n",
      "    Agent votes: S1: 2, S2: 1, S5: 1, S4: 1\n",
      "  Day 151: Strategy1 (confidence: 0.573)\n",
      "    Agent votes: S1: 2, S2: 1, S5: 1, S4: 1\n",
      "\n",
      "DQN Ensemble Test Performance:\n",
      "==================================================\n",
      "  total_return: -0.0509\n",
      "  annualized_return: -0.0345\n",
      "  volatility: 0.1883\n",
      "  sharpe_ratio: -0.1830\n",
      "  max_drawdown: 0.2172\n",
      "\n",
      "Strategy Selection Analysis:\n",
      "==================================================\n",
      "  Strategy1: 250 times (100.0%)\n",
      "\n",
      "Agent Decision Analysis:\n",
      "==================================================\n",
      "  Conservative agents:\n",
      "    Strategy1: 100.0%\n",
      "  Aggressive agents:\n",
      "    Strategy1: 100.0%\n",
      "  Momentum agents:\n",
      "    Strategy2: 74.8%\n",
      "    Strategy5: 10.4%\n",
      "    Strategy4: 9.6%\n",
      "    Strategy1: 5.2%\n",
      "  Contrarian agents:\n",
      "    Strategy5: 91.6%\n",
      "    Strategy4: 8.0%\n",
      "    Strategy1: 0.4%\n",
      "  Adaptive agents:\n",
      "    Strategy4: 98.0%\n",
      "    Strategy2: 2.0%\n",
      "\n",
      "Test results saved:\n",
      "  - Performance metrics: backtest_performance_metrics_test_dqn.csv\n",
      "  - Ensemble weights: ensemble_weights_test_dqn.csv\n",
      "  - Daily performance: ensemble_performance_test_dqn.csv\n",
      "  - Agent decisions: agent_decisions_test_dqn.csv\n",
      "\n",
      "Fixed DQN Performance:\n",
      "  total_return: -0.0509\n",
      "  annualized_return: -0.0345\n",
      "  volatility: 0.1883\n",
      "  sharpe_ratio: -0.1830\n",
      "  max_drawdown: 0.2172\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def load_and_test_model(model_path, test_data_path='cross_val_data.csv', method='DQN'):\n",
    "    \"\"\"\n",
    "    Load pre-trained ensemble model and test on test data only\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the saved model pickle file\n",
    "        test_data_path: Path to test data CSV\n",
    "        method: Method used ('DQN' or 'PPO')\n",
    "    \"\"\"\n",
    "    print(f\"=== LOADING AND TESTING {method} MODEL ===\")\n",
    "    \n",
    "    # Load the pre-trained model\n",
    "    print(f\"Loading model from: {model_path}\")\n",
    "    try:\n",
    "        with open(model_path, 'rb') as f:\n",
    "            model_data = pickle.load(f)\n",
    "        \n",
    "        rl_selector = model_data['rl_selector']\n",
    "        model_method = model_data.get('method', method)\n",
    "        print(f\"Model loaded successfully! Method: {model_method}\")\n",
    "        print(f\"Number of agents: {len(rl_selector.agents)}\")\n",
    "        \n",
    "        # CRITICAL FIX: Set all agents to evaluation mode (no exploration)\n",
    "        if method == 'DQN':\n",
    "            for agent_name, agent_info in rl_selector.agents.items():\n",
    "                # Force epsilon to 0 for pure exploitation during testing\n",
    "                agent_info['dqn'].epsilon = 0.0\n",
    "                print(f\"  {agent_name}: epsilon set to {agent_info['dqn'].epsilon}\")\n",
    "        \n",
    "        # Print agent information\n",
    "        for agent_name, agent_info in rl_selector.agents.items():\n",
    "            personality = agent_info.get('personality', 'unknown')\n",
    "            total_games = agent_info.get('total_games', 0)\n",
    "            wins = agent_info.get('wins', 0)\n",
    "            win_rate = wins / total_games if total_games > 0 else 0\n",
    "            print(f\"  {agent_name}: {personality} personality, {total_games} games, {win_rate:.2%} win rate\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Load test data\n",
    "    print(f\"\\nLoading test data from: {test_data_path}\")\n",
    "    test_data = pd.read_csv(test_data_path)\n",
    "    test_data = test_data.iloc[:, 1:]  # Remove first column if it's index\n",
    "    print(f\"Test data shape: {test_data.shape}\")\n",
    "    \n",
    "    # TESTING PHASE ONLY\n",
    "    print(\"\\n--- TESTING PHASE ---\")\n",
    "    test_strategies = TradingStrategies(test_data)\n",
    "    test_strategies.prepare_data()\n",
    "    \n",
    "    test_backtester = StrategyBacktester(test_data, test_strategies)\n",
    "    \n",
    "    # Apply trained agents to test data\n",
    "    ensemble_weights = []\n",
    "    ensemble_dates = []\n",
    "    strategy_selections = []\n",
    "    prediction_confidences = []\n",
    "    agent_decisions = []  # Track which agents made decisions\n",
    "    \n",
    "    print(\"Generating predictions with pre-trained agents...\")\n",
    "    \n",
    "    LOOKBACK_REQUIRED = 250\n",
    "    \n",
    "    if len(test_strategies.dates) < LOOKBACK_REQUIRED:\n",
    "        raise ValueError(f\"Test data insufficient. Need at least {LOOKBACK_REQUIRED} days\")\n",
    "    \n",
    "    for i in range(LOOKBACK_REQUIRED, len(test_strategies.dates)):\n",
    "        date = test_strategies.dates[i]\n",
    "        \n",
    "        # Get prediction from agent ensemble with FIXED prediction method\n",
    "        strategy_idx, confidence = predict_strategy_deterministic(rl_selector, test_strategies, i, method)\n",
    "        strategy_name = f'Strategy{strategy_idx + 1}'\n",
    "        \n",
    "        # Get detailed agent decisions for this step\n",
    "        state = rl_selector.get_market_state_vector(test_strategies, i)\n",
    "        agent_votes = {}\n",
    "        \n",
    "        for agent_name, agent_info in rl_selector.agents.items():\n",
    "            if method == 'DQN':\n",
    "                # Ensure epsilon is 0 and use deterministic action selection\n",
    "                agent_info['dqn'].epsilon = 0.0\n",
    "                action = agent_info['dqn'].act(state, agent_info['personality_bias'])\n",
    "            else:  # PPO\n",
    "                action, _, _ = agent_info['ppo'].get_action_and_value(state, agent_info['personality_bias'])\n",
    "            \n",
    "            agent_votes[agent_name] = {\n",
    "                'strategy': action,\n",
    "                'personality': agent_info['personality']\n",
    "            }\n",
    "        \n",
    "        # Get weights using selected strategy\n",
    "        strategy_func = getattr(test_strategies, f'task1_{strategy_name}')\n",
    "        weights = strategy_func(i)\n",
    "        \n",
    "        ensemble_weights.append(weights.values)\n",
    "        ensemble_dates.append(date)\n",
    "        strategy_selections.append(strategy_name)\n",
    "        prediction_confidences.append(confidence)\n",
    "        agent_decisions.append(agent_votes)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            day_num = i - LOOKBACK_REQUIRED + 1\n",
    "            print(f\"  Day {day_num}: {strategy_name} (confidence: {confidence:.3f})\")\n",
    "            \n",
    "            # Show agent consensus for this prediction\n",
    "            strategy_counts = defaultdict(int)\n",
    "            for agent_name, decision in agent_votes.items():\n",
    "                strategy_counts[f\"S{decision['strategy']+1}\"] += 1\n",
    "            \n",
    "            consensus_str = \", \".join([f\"{strat}: {count}\" for strat, count in strategy_counts.items()])\n",
    "            print(f\"    Agent votes: {consensus_str}\")\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    ensemble_weights_df = pd.DataFrame(\n",
    "        ensemble_weights,\n",
    "        index=ensemble_dates,\n",
    "        columns=test_strategies.symbols\n",
    "    )\n",
    "    \n",
    "    # Calculate performance\n",
    "    ensemble_returns = test_backtester.calculate_returns(ensemble_weights_df)\n",
    "    ensemble_performance = test_backtester.calculate_performance_metrics(ensemble_returns)\n",
    "    \n",
    "    print(f\"\\n{method} Ensemble Test Performance:\")\n",
    "    print(\"=\" * 50)\n",
    "    for metric, value in ensemble_performance.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    \n",
    "    # Analyze strategy selection patterns\n",
    "    print(f\"\\nStrategy Selection Analysis:\")\n",
    "    print(\"=\" * 50)\n",
    "    strategy_counts = pd.Series(strategy_selections).value_counts()\n",
    "    for strategy, count in strategy_counts.items():\n",
    "        percentage = (count / len(strategy_selections)) * 100\n",
    "        print(f\"  {strategy}: {count} times ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Analyze agent consensus\n",
    "    print(f\"\\nAgent Decision Analysis:\")\n",
    "    print(\"=\" * 50)\n",
    "    personality_decisions = defaultdict(list)\n",
    "    for decisions in agent_decisions:\n",
    "        for agent_name, decision in decisions.items():\n",
    "            personality_decisions[decision['personality']].append(decision['strategy'])\n",
    "    \n",
    "    for personality, decisions in personality_decisions.items():\n",
    "        strategy_dist = pd.Series(decisions).value_counts()\n",
    "        total = len(decisions)\n",
    "        print(f\"  {personality.title()} agents:\")\n",
    "        for strategy_idx, count in strategy_dist.items():\n",
    "            percentage = (count / total) * 100\n",
    "            print(f\"    Strategy{strategy_idx+1}: {percentage:.1f}%\")\n",
    "    \n",
    "    # Save test results\n",
    "    test_suffix = f\"test_{method.lower()}\"\n",
    "    \n",
    "    ensemble_performance_df = pd.DataFrame([ensemble_performance])\n",
    "    ensemble_performance_df.to_csv(f'backtest_performance_metrics_{test_suffix}.csv', index=False)\n",
    "    \n",
    "    ensemble_weights_df.to_csv(f'ensemble_weights_{test_suffix}.csv')\n",
    "    \n",
    "    performance_df = pd.DataFrame({\n",
    "        'Date': ensemble_dates,\n",
    "        'Returns': ensemble_returns.values,\n",
    "        'Cumulative_Returns': (1 + ensemble_returns).cumprod().values,\n",
    "        'Selected_Strategy': strategy_selections,\n",
    "        'Prediction_Confidence': prediction_confidences\n",
    "    })\n",
    "    performance_df.to_csv(f'ensemble_performance_{test_suffix}.csv', index=False)\n",
    "    \n",
    "    # Save detailed agent decisions\n",
    "    agent_decisions_df = []\n",
    "    for i, (date, decisions) in enumerate(zip(ensemble_dates, agent_decisions)):\n",
    "        for agent_name, decision in decisions.items():\n",
    "            agent_decisions_df.append({\n",
    "                'Date': date,\n",
    "                'Day': i + 1,\n",
    "                'Agent': agent_name,\n",
    "                'Personality': decision['personality'],\n",
    "                'Selected_Strategy': f\"Strategy{decision['strategy']+1}\",\n",
    "                'Ensemble_Strategy': strategy_selections[i],\n",
    "                'Confidence': prediction_confidences[i]\n",
    "            })\n",
    "    \n",
    "    agent_decisions_df = pd.DataFrame(agent_decisions_df)\n",
    "    agent_decisions_df.to_csv(f'agent_decisions_{test_suffix}.csv', index=False)\n",
    "    \n",
    "    print(f\"\\nTest results saved:\")\n",
    "    print(f\"  - Performance metrics: backtest_performance_metrics_{test_suffix}.csv\")\n",
    "    print(f\"  - Ensemble weights: ensemble_weights_{test_suffix}.csv\")\n",
    "    print(f\"  - Daily performance: ensemble_performance_{test_suffix}.csv\")\n",
    "    print(f\"  - Agent decisions: agent_decisions_{test_suffix}.csv\")\n",
    "    \n",
    "    return ensemble_weights_df, ensemble_performance, agent_decisions_df\n",
    "\n",
    "def predict_strategy_deterministic(rl_selector, strategies, current_idx, method):\n",
    "    \"\"\"\n",
    "    Deterministic prediction method that matches the original training behavior\n",
    "    \"\"\"\n",
    "    state = rl_selector.get_market_state_vector(strategies, current_idx)\n",
    "    \n",
    "    strategy_votes = defaultdict(float)\n",
    "    \n",
    "    for agent_name, agent_info in rl_selector.agents.items():\n",
    "        if method == 'DQN':\n",
    "            # Force deterministic behavior (no exploration)\n",
    "            agent_info['dqn'].epsilon = 0.0\n",
    "            action = agent_info['dqn'].act(state, agent_info['personality_bias'])\n",
    "        else:  # PPO\n",
    "            action, _, _ = agent_info['ppo'].get_action_and_value(state, agent_info['personality_bias'])\n",
    "        \n",
    "        # Weight by recent performance\n",
    "        weight = np.mean(agent_info['recent_rewards']) if agent_info['recent_rewards'] else 0\n",
    "        weight = max(0.1, weight)\n",
    "        \n",
    "        strategy_votes[action] += weight\n",
    "    \n",
    "    if strategy_votes:\n",
    "        best_strategy = max(strategy_votes.items(), key=lambda x: x[1])[0]\n",
    "        confidence = strategy_votes[best_strategy] / sum(strategy_votes.values())\n",
    "        return best_strategy, confidence\n",
    "    else:\n",
    "        return 0, 0.2\n",
    "\n",
    "def fix_dqn_model_for_testing(model_path, output_path=None):\n",
    "    \"\"\"\n",
    "    Load a DQN model and fix it for consistent testing behavior\n",
    "    \"\"\"\n",
    "    if output_path is None:\n",
    "        output_path = model_path.replace('.pkl', '_fixed.pkl')\n",
    "    \n",
    "    print(f\"Fixing DQN model: {model_path}\")\n",
    "    \n",
    "    with open(model_path, 'rb') as f:\n",
    "        model_data = pickle.load(f)\n",
    "    \n",
    "    rl_selector = model_data['rl_selector']\n",
    "    \n",
    "    # Fix all DQN agents\n",
    "    for agent_name, agent_info in rl_selector.agents.items():\n",
    "        if 'dqn' in agent_info:\n",
    "            # Set epsilon to 0 for deterministic behavior\n",
    "            agent_info['dqn'].epsilon = 0.0\n",
    "            print(f\"  Fixed {agent_name}: epsilon = {agent_info['dqn'].epsilon}\")\n",
    "    \n",
    "    # Save fixed model\n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump(model_data, f)\n",
    "    \n",
    "    print(f\"Fixed model saved to: {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "# Usage functions\n",
    "def test_fixed_model(model_path='ensemble_model_dqn.pkl', method='DQN'):\n",
    "    \"\"\"\n",
    "    Test model with guaranteed consistent behavior\n",
    "    \"\"\"\n",
    "    if method == 'DQN':\n",
    "        # First fix the model\n",
    "        fixed_model_path = fix_dqn_model_for_testing(model_path)\n",
    "        return load_and_test_model(fixed_model_path, method=method)\n",
    "    else:\n",
    "        return load_and_test_model(model_path, method=method)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Testing DQN model with fixed epsilon...\")\n",
    "    dqn_weights, dqn_performance, dqn_decisions = test_fixed_model(\n",
    "        model_path='ensemble_model_dqn.pkl',\n",
    "        method='DQN'\n",
    "    )\n",
    "    \n",
    "    print(\"\\nFixed DQN Performance:\")\n",
    "    for metric, value in dqn_performance.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
